---
layout: page-with-sidebar-math
title:  "Filter往事(一)站在巨人的肩膀上"
date:   2024-01-30 10:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: "预研"
---

## 序

Bloom Filter也许很多人都知道，原理也很简单，实现起来更是简单。之前我对此也不以为然，好像也就是一个平平无奇的算法。后来听了教练组的一节课，才发现我们以为平平无奇的东西，只是因为我们站在了巨人的肩膀上。而巨人又站在别的巨人的肩膀上，于是我们越走越远。

有人单靠一手Bloom FIlter走到了教授的位置，现如今Bloom Filter的变种，以及各种优化版本已经数不胜数，形成了一个专门的研究分类，也出现了越来越复杂的新版FIlter。我研究FIlter也就两个月，不指望能有多高的造诣，只当是拾人牙慧好了。



## Cardinality 问题

计算机最古老的问题之一；给你一个数和一个集合，查询这个数被添加到了这个集合多少次

有比较著名的算法 `Count-min sketch`、`Counting Bloom Filter`。但不在本次讨论范围内。



## Membership Query 问题

计算机最古老的问题之一；给你一个数和一个集合，查询这个数在不在这个集合中。

最初的解决方案（naive approach）：存储所有数

可以使用以下数据结构：

1、List：$$O(1)$$插入、$$O(n)$$删除、$$O(n)$$查询

2、Sorted List：$$O(n)$$插入、$$O(n)$$删除、$$O(logn)$$查询

3、二叉搜索树（AVL、RB）：$$O(logn)$$插入、$$O(logn)$$删除、$$O(logn)$$​查询

有[论文](https://dl.acm.org/doi/10.1145/322261.322274)证明了二叉树更优

或使用hash映射，下面主要探讨各种hash的特点与优劣。



### Hash桶 -> 链地址法（chaining）

通过hash将key映射到不同的桶中，实现上把key直接存储到对应的桶中。hash冲撞的概率直接影响效率，在最坏情况下，所有key都被映射到一个桶中，此时查询、插入、删除都退化成最初的解决方案，甚至每个桶内可以自成一个二叉搜索树。可以说兼具了初始方案优点的同时，通过分组进一步减小了查询时间。

`上面的解决思路是没有错误率的，也就是查询、插入、删除的结果一定是正确的！`

`但在缓存等场景，其实我们可以允许存在False Positive（假阳率），但是不允许Flase negative（假阴率）。因此有诞生了以下Filter`



### Hash桶->演化一（FingerPrint）

最初版本的Hash桶里面存着的是key本身，但是现在我们考虑将key映射到（1～m）的集合中，小于等于m的数都可以用$$(log_2m)$$位表示其二进制数。假设我们插入n个key，那么fingerPrint版本的 space 开销为$$n(log_2m)$$。

而Hash桶的空间开销为 $$nm$$ ，看得出来哈，通过FingerPrint，我们成功减小了空间开销，当然也引入了错误率。只有当两个key的Hash值出现碰撞时，我们才会出现错误场景（False Positive）。

对于计算方面，每一次操作前，多了一个Hash计算，其他并没有变。计算量小幅度增加。

`总结：通过fingerPrint的引入，我们进入了 概率数据结构（Probabilistic Data Structure）的领域（realm）。`



### Hash桶->演化二（FingerPrint）开放寻址法（open addressing）QuotientFilter

开放寻址法和链地址法都是解决hash冲突的方法。

QuotientFilter（开放寻址法）：

```
大致要做两次hash，首先将key做第一次hash得到index，然后再做第二次hash（fingerPrint），将第二次hash的值存入index所在位置。如果该位置已经存在元素，则往下顺延一个位置，直到找到空位。

因此我们在查询的时候也需要对两个Hash做比较。一个是index，一个是 fingerPrint。

但是在查找的时候我们会比较困惑，当我们找到指纹的时候，他到底是顺延下来的，还是直接加入的？这会让我们无法判断他对应的index是什么。于是需要我们用额外的metadata来存储index的信息。

这就是QuotientFilter，支持删除，但是比较慢，还有额外的metadat空间，一般用于外存等大存储硬件。

Quotient是除法的意思，这是由于我们可以通过计算一次大Hash，然后通过除法得到两个小的hash值，分别作为index和fingerPrint。由于这种实现方式，所以命名为QuotientFilter。
```

实际的优化点：

```
最开始的Hash桶是一个HashMap，有一个mapKey，对应一个链表作为value。当我们新增一个key进入Hash桶时，做两次hash得到一个index和一个fingerPrint。此时我们把index位做mapKey，获取到对应的链表value，将我们的fingerprint插入链表中。

而现在，我们不需要用Hashmap了，因为index可以看作是一个偏移量，这样我们就不用单独存储他了。我们从开头位置找到偏移index位置的地方，插入fingerprint，如果该位置已经存在元素，则往下顺延一个位置，直到找到空位。

还有一个显而易见的优点，fingerPrint被顺序存储起来，我们在query的时候，属于顺序读，效率会高一些。
```



### Hash桶->演化三（FingerPrint）CuckooFilter

我们考虑使用两个QuotientFilter的数据结构组合起来，通过两个结构互相映射，达到查找index的目的，这样就可以不用存储metadata了。这就是Cuckoo Filfter（2014年）支持删除

具体为：

```
假设两个数据结构为D1和D2
插入一个key时：首先计算index和fingerprint
1、首先查看D1[index]的位置是不是空的
	（1）如果是就直接放进去。
	（2）如果不是...就经过一些鸠占鹊巢的操作。。太难理解了。。先不管了
```

同样有一个优化，可以将两个短的数据结构合并成一个长的，然后两个数据结构共用它，类似下面的“合并独立多位图”，他的效果要好一些。



### FingerPrint的更优解 -> Bitmap（单位图）

关于FingerPrint，更天才的做法是使用位图。

我们考虑将key映射到（1～m）的集合中，那我们就准备一个长为m的位图，当添加key时，就把key对应的Hash值当成位图的下标，将下标对应的位 置为1。

这样space开销为 $$m$$

计算FP:

在查询key时，有多大的概率出现Hash碰撞。也就是有多大的概率，明明这个key没有被插入过，但是其对应的位图上已经被置为1。

假设本次查询的key的hash值为t。位图已经插入了n个不同于key的数。位图长度为m。

```
概率要怎么算，其实这里也有点技巧
我们想求的FP实际上就是对于一个新的key计算出来的hash值t，之前的n次添加中，已经添加过的概率。
但是我们最好从没有添加的概率入手，最后用1-没有添加的概率
为什么这么算？
1、假如我们使用添加的概率计算，如果有两次添加操作，假设两次添加的概率都为P1
那么两次之后t被添加的概率等于（第一次添加的概率）加上（第一次没有添加的概率 * 第二次添加的概率）：P1 + (1-P1)P1
因为我们要新增第一次没有添加，而第二次添加了的概率
2、如果我们使用没有添加的概率计算，如果有两次添加操作，假设两次没有添加的概率都为P2
那么两次都添加的概率为：1- P2^2

假设有三次操作，那么使用添加的概率计算就会越来越麻烦。所以我们使用1-没有添加的概率来计算

这里面确实比较绕。。。，核心点在于我们的需求是什么，
1、如果要求的事件 只需发生至少一次，那么我们就反过来算。
2、如果要求的事件 必须每次都发生，我们就正着算。
```

1、第一次插入，没有插到t位置的概率为 $(1- {1 \over m})$

2、第n次插入，没有插到t位置的概率为 $(1- {1 \over m})^n$

3、于是我们得到了第一个FP的计算公式： $\epsilon=1- (1- {1 \over m})^n$

下面通过高等数学手段对上面的表达式作化简：

`至于为什么要化简，是因为目前有三个变量，fp、m、n 这时我们只能通过三维图像来表示他们的关系。但是化简之后，我们将n/m看成一个变量，这样就从三个变量化简为了两个变量，起到了降维的作用，方便分析。`

$(1- {1 \over m})^n$

 = $ (1- {1 \over m})^{m * {n \over m}}$ 

= $((1- {1 \over m})^m) ^ {n \over m}$

令：$y = (1- {1 \over m})^m$

1️⃣ 两边取对数：$ ln(y) = ln((1- {1 \over m})^m) = m*ln(1-{1 \over m})$

泰勒展开：$ln(1-{1 \over m}) = -{1 \over m} - {1 \over 2m^2} - {1 \over 3m^3} - ...$

带入1️⃣，得到：$ln(y) = -{1} - {1 \over 2m} - {1 \over 3m^2} - ...$

当m趋于无穷大，高阶项可以被忽略，因此 $\lim\limits_{m→∞} ln(y) \approx -1$

于是：$\lim\limits_{m→∞} y \approx e^{-1}$

于是：$\lim\limits_{m→∞} (1- {1 \over m})^m \approx e^{-1}$

于是：$\lim\limits_{m→∞} ((1- {1 \over m})^m)^{n \over m} \approx (e^{-1})^{n \over m}$

于是：$\lim\limits_{m→∞} (1- {1 \over m})^n \approx e^{ - {n \over m}}$



于是我们得出 $FP \approx 1- e^{ - {n \over m}}$​

结合GeoGebra Graphing Calculator，我们可以得到，在不同n和m的比值下，fp的具体值。

![image-20240131104116231](/assets/images/2024-01-30-Filter往事(一)站在巨人的肩膀上//image-20240131104116231.png)

当m和n相等时，fp约为63%

当m是n的10倍时，fp约为9.5%

当m是n的100倍时，fp约为1%

为了方便，我把$m \over n$定义为x，便于观察`空间负载`是`插入key数量`的多少倍。通过图像看出，`随着空间成倍增加，fp的降低的越来越慢。`

![image-20240131105608118](/assets/images/2024-01-30-Filter往事(一)站在巨人的肩膀上//image-20240131105608118.png)



### Bitmap再优化 -> 独立多位图（independent bitmap）

当$m = 100n$时，$FP \approx 1\%$

如果我们增加空间到200n，根据公式可以得出$FP \approx 0.5\%$



假如我们不把200n看作是一个位图，而看成是两个独立的位图。单个位图的FP为1%。那么现在我新增一个key，这个key在第一个位图中发生Hash碰撞的概率为1%，在第二个发生Hash碰撞的概率也为1%。假如key在两个位图发生Hash碰撞是相互独立的事件，那么根据统计学原理，新的key在两个位图同时发生Hash碰撞的概率为$\%1 * \%1 = 0.01\%$，远小于0.5%！

因此使用多位个独立位图（Hash函数之间互相独立），会大大优化空间占用，并且达到较低的FP。

那么是不是我们将一个整体分成的独立位图数越多越好呢？也不是，下面从数学角度证明。

还记得上面我们得到的fp公式吗？ $FP = 1- (1- {1 \over m})^n$

现在我们引入一个新的变量k，假设我们把m的长度分成k个相互独立的位图，那么FP应该怎么算？

`我们先将问题分解，先计算第一个位图的FP，这样除了长度变小了，其他都没变，于是可以直接套用之前的技巧思路得到:`

其中一个位图冲突的概率：$FP=1-(1-{1\over{m \over k}})^n$

有k个这样的位图，他们全都冲突的概率就是最终的FP：$FP = (1- (1- {1 \over (m / k)})^n)^k$​

令：$y = (1- {k \over m})^m$

1️⃣ 两边取对数：$ ln(y) = ln((1- {k \over m})^m) = m*ln(1-{k \over m})$

泰勒展开：$ln(1-{k \over m}) = -{k \over m} - {k^2 \over 2m^2} - {k^3 \over 3m^3} - ...$

带入1️⃣，得到：$ln(y) = -{k} - {k^2 \over 2m} - {k^3 \over 3m^2} - ...$

当m趋于无穷大，高阶项可以被忽略，因此 $\lim\limits_{m→∞} ln(y) \approx -k$​

于是：$\lim\limits_{m→∞} y \approx e^{-k}$ => $(1- {k \over m})^m \approx e^{-k}$

$FP = (1- ((1- {k \over m})^m)^{n \over m})^k$​

$\lim\limits_{m→∞}FP \approx \lim\limits_{m→∞} (1 - e^{-{nk \over m}})^k$



通过和上面一样的推理思路，我们可以得到$\lim\limits_{m→∞} (1-(1- {k \over m})^n)^k \approx (1- e^{ - {nk \over m}})^k$

观察公式，随着k的增加，

1、由于括号内的内容一定是一个小于1的正数，所以外层k次方，k越大，值越小

2、对于内层，k越大，$1-e^{-{nk \over m}}$越大

因此对于k而言，FP并不是一个单调函数

下面对$(1- e^{ - {nk \over m}})^k$求极值

设：$y = (1- e^{ - {nk \over m}})^k$

取对数：$ln(y) = ln ((1- e^{ - {nk \over m}})^k) = kln(1-e^{-{nk \over m}})$

求导：

由于y是一个关于k的函数，左侧看作是一个复合函数，复合函数求导等于外层导乘以内层导

一个关于k的函数乘以另一个关于k的复合函数

$ln^{'}(x) = {1 \over x}$

${1\over y }{\partial y \over \partial k} = ln(1-e^{-{nk \over m}}) + k{ ne^{-{nk \over m}} \over m(1-e^{-{nk \over m}})}$

将$y = (1- e^{ - {nk \over m}})^k$ 带入得到：

${\partial y \over \partial k} = (1- e^{ - {nk \over m}})^k*(ln(1-e^{-{nk \over m}}) + k{ ne^{-{nk \over m}} \over m(1-e^{-{nk \over m}})})$

当y对于k的偏导数等于零时，k的值可能为极值点

需要计算：$ln(1-e^{-{nk \over m}}) + k{ ne^{-{nk \over m}} \over m(1-e^{-{nk \over m}})} = 0$

这玩意不光我不会解，ChatGPT也不会，但是把结果带回去算一下是对的，$k={m \over n}ln2$，至于怎么解的，我放弃了。。

可知当$k={m \over n}ln2$时，FP取到极小值。

带入公式：$(1- e^{ - {nk \over m}})^k = ({1 \over 2})^{mln2 \over n}$

于是空间开销$m = - ({nlog_2\epsilon \over ln2}) \approx -1.44nlog_2\epsilon$



### 独立多位图的更优版本 -> 合并独立多位图（merge the bitmaps）

上面我们把空间m，平均分成了k个互相独立的位图，每个位图的长度为$m \over k$。

但是我们考虑如果这些位图可以重合呢？那么我们就得到了k个长度为m的位图，只不过这k个位图共用同一块空间m。

`这就是当前主流的BloomFilter的实现方式了`

每次添加key之前，会通过k个独立的Hash函数分别计算Hash值，那么这个key最多有k个位置会被设置为1。查询时需要这几个位置同时为1，才证明key可能存在。那么这时的误判率是多少呢？

`假设新增的key对应k个Hash值分别为h1、h2、h3...hk；我们还是先将问题分解，先计算h1冲突的概率；思路还是参考上面的思路，先求不冲突的概率，再用1减去不冲突的概率。`

$P(h1) = 1 -(1-{1 \over m})^{nk}$

`计算出了h1冲突的概率之后，h2、h3等等都是一样的概率，总共有k个位置（其实也可能小于k，如果两个Hash函数对key做计算之后得到同一个值，那么这个位置数量就会-1，但是这个概率很小，忽略不计。。。）。那么这k个位置都冲突的概率为`

$FP = P(h1)*P(h2)*...*P(hk) = (1 -(1-{1 \over m})^{nk})^k$​

前面已经算过了$\lim\limits_{m→∞} (1- {1 \over m})^m \approx e^{-1}$

$FP = (1 -((1-{1 \over m})^m)^{nk \over m})^k$

$\lim\limits_{m→∞}FP \approx \lim\limits_{m→∞} (1 - e^{-{nk \over m}})^k$​

发现和独立多位图的渐进形式一模一样，但是合并独立多位图要更优一些。为什么？

我给一个比较抽象的理解：

1、独立多位图：

关键计算步骤：

$ln((1- {k \over m})^m) = -{k} - {k^2 \over 2m} - {k^3 \over 3m^2} - ...$

 $\lim\limits_{m→∞} (1- {k \over m})^m = e^{-k- {k^2 \over 2m} - {k^3 \over 3m^2} - ...}$

2、合并独立多位图：

$ln((1- {1 \over m})^m) = -{1} - {1 \over 2m} - {1 \over 3m^2} - ...$

$\lim\limits_{m→∞} (1- {1 \over m})^m = e^{-{1} - {1 \over 2m} - {1 \over 3m^2} - ...}$​

对比两个极限表达式，对于独立多位图，我们忽略的是$ - {k^2 \over 2m} - {k^3 \over 3m^2} - ...$，而对于合并独立多位图，我们忽略的是$ - {1 \over 2m} - {1 \over 3m^2} - ...$，比较明显，独立多位图被忽略的部分的绝对值更大。也就是说，取极限之后，两个式子是一样的，但是如果把忽略的部分给减回去，独立多位图的FP会更大一些。（虽然我不太认可这种解释，很牵强的感觉，应该有别的更严谨的证明。但是结论应该是对的。）



### Counting Bloom Filter

可以看出来Bloom Filter不支持删除，因为Hash值冲突的时候，我们使用了 或操作 去添加元素。专业来讲就是说他不是linear的。

为了支持删除，我们考虑将原来的m个单bit位变成m个多bit位。例如2个比特位最多能表示4个数，那么我们就可以最多添加4个同样的Hash值，这样也就可以删除了。

这就是Counting Bloom Filter。

考虑我们原来的空间m中每一个bit位表示一个hash值是否存在，现在我们用z个bit位表示一个hash值被添加的次数，每一个z个bit位能表示$2^z$个次数。这样实际我们能插入的key的数量也应该处以z。

原来能插入n个key，现在只能插入$n \over z$​个key了。

通常我们的$z=4$​就够了



### 完美Hash与静态函数

完美Hash函数：没有Hash碰撞的Hash函数，但是很明显，存在完美Hash的前提是你已知了所有要添加的key。

静态数据：由于已知了所有的key。这个key的集合，我们称为是静态的

假设一个Hash函数可以把n个k完美的分到【1，u】的区间，或者说【0，2^j -1】

下面的计算 我们使用Hash桶开放寻址法的算法进行计算：

total space m = n * j

$FP = {1 \over 2^j}$​

=> $m = -nlog_2 \epsilon$

而BloomFilter的空间开销$m = - ({nlog_2\epsilon \over ln2}) \approx -1.44nlog_2\epsilon$

由此得到BloomFilter是完美hash的理论最小空间开销的1.44倍

完美Hash是针对Hash桶开放寻址法定义的，为什么不针对Bitmap

```
我们考虑使用bitmap，假设空间为m，插入n个key。这些key被完美映射到n个不同的bit位。
空间消耗：m
FP = n / m

我们对比上面的开放寻址法，当FP = 0.25时：
1、bitmap： m = 4n
2、开放寻址：j=2 => m = 2n

可以看到在相同FP下，开放寻址空间更少，而且随着FP进一步降低，这个差距会越来越大。
因此在计算理论最小值时，我们使用开放寻址法。
```



### 生日悖论

假如一个班级里面，只有23个人，那么至少有两个人的生日在同一天的概率，居然高达50%。这一反直觉的概率问题，称为生日悖论。

根据我们前面的总结的概率计算技巧：
1、如果要求的事件 只需发生至少一次，那么我们就反过来算。
2、如果要求的事件 必须每次都发生，我们就正着算。

这里我们反过来算

$P = 1 - ({365 \over 365} * {364 \over 365} * {363 \over 365} * ... *{365 - n + 1 \over 365})$

= $1 - ({365! \over 365^n * (365-n)!})$

当n=23时，计算$P \approx 50\%



### [XOR Filter](https://arxiv.org/abs/1912.08258)

XOR的发音：`/ɛks ˈɔː(ɹ)/` 或 `/ˈzɔː(ɹ)/`，hvv讲师会读第二个发音。

根据生日悖论，我们可以发现，找到完美Hash是非常难的。于是我们换个思路，我们允许Hash碰撞的存在，我们通过将key映射到多个位置，通过贪心算法，找到这多个位置中，独属于key的位置，并插入。经过n次迭代，我们就完成了插入n个key的操作。但是很明显，插入操作是可能失败的。因为可能我们映射的多个位置都已经被占用了，这时插入失败。

在Xor Filter中插入操作也称为peeling。关于Xor Filter的实现过程，有一个有趣说法是：`类似于超图的拓扑排序`。

业界如Lindorm，曾将Bloom Filter替换为Ribbon Filter提升缓存命中率。Ribbon Filter是由Xor Filter优化而来。所以Xor Filter需要作为前置条件理解一下。

但是看了知乎、CSDN好几篇帖子，写的相当抽象，我就不给引用了。说他们写的不认真吧，还画了很多图；说他们写的认真吧，细节很多错误；并且很多段落是一样的，感觉都是互相抄的；理解上的易错点也没有讲出来，看完还是云里雾里的。

无奈Google到了[论文](https://arxiv.org/pdf/1912.08258.pdf)，自己啃一遍吧。啃论文是真不容易，但是啃完也是能懂得。主要是结合伪代码理解，豁然开朗。talk is cheap，show me the code`是有道理的。

几个理解上的关键难点：

#### 论文里面那些符号怎么理解？

![image-20231121160157285](/assets/images/2023-11-21-过滤器优化//image-20231121160157285.png)

> U：表示所有可能作为Filter输入的元素
>
> S：表示你要对哪一个集合构建过滤器，Bloom过滤器需要提前构造，需要把已知的数据映射到过滤器中。这个提前构造可能很多人没概念。
>
> \|S\|：目标集合的大小
>
> B：Xor过滤器最终存在形态就是这么一个数组，数组里面存的是k位的数值
>
> c = \|B\|：过滤器的长度，论文建议长度由(1.23*\|S\|)+23计算得来
>
> fingerprint：也是个hash函数，可以将U中任意元素，映射为一个固定长度为k位的数值。实现上可以参考：比如把任意元素分成三组，3的机器码为11，因此你总可以得到固定长度为2位的数值。例如1对应01，2对应10，3对应11。
>
> h1、h2、h3:三个hash函数，分别将U中的元素映射到[0, c/3]、[c/3, 2c/3]、[2c/3, c]

#### 如何构造Xor FIlter

主要看以下三个伪代码：Algorithm2、3、4

![image-20231121163234079](/assets/images/2023-11-21-过滤器优化//image-20231121163234079.png)

下面这个Algorithm 3 有几率失败。

![image-20231121163506675](/assets/images/2023-11-21-过滤器优化//image-20231121163506675.png)

论文给出了失败的概率统计，可以看到当size越来越大时，成功率接近100%

![image-20231121163601434](/assets/images/2023-11-21-过滤器优化//image-20231121163601434.png)

![image-20231121163643314](/assets/images/2023-11-21-过滤器优化//image-20231121163643314.png)

#### 如何判断元素是否存在？

![image-20231121163727997](/assets/images/2023-11-21-过滤器优化//image-20231121163727997.png)

#### 为什么这样的算法可以用来判断元素存在？

首先我们需要了解异或运算本身的性质

异或运算有：

1、交换律

2、结合律

3、两个相同数xor的结果总为0

4、任何数和0 xor总为数本身

Algorithm2、3、4这三个算法保证了：

`对于任意的数x, B[h0(x)]、B[h1(x)]、B[h2(x)]中有且仅有一个apply了Algorithm4中的下面的赋值语句`

至于为什么能保证，请结合算法本身理解，这一部分的理解可以参考[知乎里面的图解](https://zhuanlan.zhihu.com/p/543943112)

![image-20231121165802686](/assets/images/2023-11-21-过滤器优化//image-20231121165802686.png)

假如只有B[h0(x)] apply 了上面的赋值语句，也就是i=h0(x)的情况，有 

`B[h0(x)] xor B[h1(x)] xor B[h2(x)] `

 `= `

 `(fingerprint(x) xor B[h0(x)] xor B[h1(x)] xor B[h2(x)]) `

`xor B[h1(x)]`

`xor B[h2(x)]`

`=`

`fingerprint(x) xor B[h0(x)] xor B[h1(x)] xor B[h2(x)] xor B[h1(x)] xor B[h2(x)]`

`=`

`fingerprint(x) xor B[h0(x)]`

`=fingerprint(x)    #由于i=h0(x)，所以B[h0(x)]被赋值为0；`



#### Xor Filter的另外一种角度（perspective）

Xor Filter peeling的过程其实可以看成是矩阵求解。



