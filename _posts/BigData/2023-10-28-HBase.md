---
layout: page-with-sidebar
title:  "HBase"
date:   2023-10-28 10:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
hideTag: false
---

## PE

pe是hbase官方提供的一个测试工具，可以方便的创建测试数据集。

```shell
hbase pe --nomapred --oneCon=true --table=Tab_2G_write_20 --size=2 --valueSize=20 --columns=8 --presplit=10 --compress='SNAPPY' sequentialWrite 16
// 16线程 10个预分区 8个列 2G value长度为20 压缩方式为SNAPPY
```



## [官网资料](https://hbase.apache.org/apache_hbase_reference_guide.pdf)

学习hbase，当然首选官网了。官网有一版中文的资料，但是已经相当久了，只能图一乐。

看英文文档又非常痛苦，还好现在是AI的时代，chrom里装一个 `沉浸式翻译`插件就可以愉快的读native文档了。

### 架构

RegionServer管理着多个region，建议一个RegionServer管理region的数量不超过100个，因为假如一个RegionServer管理1000个region，那么就需要3.9G的MemStore空间，这还是啥都没存的情况下。（见72.1）

region又有以下结构（见72）

![image-20231120171009295](/assets/images/2023-10-28-HBase//image-20231120171009295.png)

一个Store存着0到多个StoreFile（HFile）（72.7）

这里辟个谣，StoreFile和Hfile是一个东西，很多资料里面说HFile属于StoreFile，这个是不准确的。而StoreFile/HFile里面是Block。

可以使用hbase hfile工具查看HFile信息, 可以通过以下命令查看工具用法

```shell
hbase hfile
```

打印如下：

```shell
usage: HFile [-a] [-b] [-e] [-f <arg> | -r <arg>] [-h] [-k] [-m] [-p]
       [-s] [-v] [-w <arg>]
 -a,--checkfamily         Enable family check
 -b,--printblocks         Print block index meta data
 -e,--printkey            Print keys
 -f,--file <arg>          File to scan. Pass full-path; e.g.
                          hdfs://a:9000/hbase/hbase:meta/12/34
 -h,--printblockheaders   Print block headers for each block.
 -k,--checkrow            Enable row order check; looks for out-of-order
                          keys
 -m,--printmeta           Print meta data of file
 -p,--printkv             Print key/value pairs
 -r,--region <arg>        Region to scan. Pass region name; e.g.
                          'hbase:meta,,1'
 -s,--stats               Print statistics
 -v,--verbose             Verbose output; emits file and meta data
                          delimiters
 -w,--seekToRow <arg>     Seek to this row and print all the kvs for this
                          row only
```

HFile中包含索引（见71.4.4）、bloom过滤器，以及实际的row

`An HFile is the file format that HBase uses to store data in HDFS. It contains a multi-layered
index which allows HBase to seek the data without having to read the whole file. `

关于HFile的详细介绍，可以看下[这一篇](https://www.iteye.com/blog/bit1129-2201094)

### Bloom过滤器

见124.4

存储在HFile的元数据中，下面的命令可以看到hfile的元数据，里面有Bloom Filter，也可以看到Block index的大小信息，

```shell
hbase hfile -m -f <hfile的全路径>
```

#### 源码分析

UT起手，根据官方文档得知，Bloom是属于HFile的一部分，所以HFile的UT里面肯定有相关测试。

分析TestHFile文件中的第一个UT `testReaderWithoutBlockCache`可以清晰的看到，这个UT使用`writeStoreFile()`随机生成了一些key value，并且通过StoreFileWriter的append方法写入了他们。

往下追踪正式进入HBase写流程

```java
// StoreFileWriter.java
@Override
public void append(final Cell cell) throws IOException {
  appendGeneralBloomfilter(cell);
  appendDeleteFamilyBloomFilter(cell);
  writer.append(cell);
  trackTimestamps(cell);
}
```

可以看到在writer写入之前，先计算了bloom filter。这里有两个filter，第二个是用于过滤deleted family的，不知道这是干啥的，可以先不管。

追踪appendGeneralBloomfilter

```java
private void appendGeneralBloomfilter(final Cell cell) throws IOException {
  if (this.generalBloomFilterWriter != null) {
    /*
     * http://2.bp.blogspot.com/_Cib_A77V54U/StZMrzaKufI/AAAAAAAAADo/ZhK7bGoJdMQ/s400/KeyValue.png
     * Key = RowLen + Row + FamilyLen + Column [Family + Qualifier] + Timestamp 3 Types of
     * Filtering: 1. Row = Row 2. RowCol = Row + Qualifier 3. RowPrefixFixedLength = Fixed Length
     * Row Prefix
     */
    bloomContext.writeBloom(cell);
  }
}
```

追踪writeBloom

```java
public void writeBloom(Cell cell) throws IOException {
  // only add to the bloom filter on a new, unique key
  if (isNewKey(cell)) {
    sanityCheck(cell);
    bloomFilterWriter.append(cell);
  }
}
```

追踪bloomFilterWriter.append

```java
// CompoundBloomFilterWriter.java
@Override
public void append(Cell cell) throws IOException {
  Objects.requireNonNull(cell);

  enqueueReadyChunk(false);

  if (chunk == null) {
    if (firstKeyInChunk != null) {
      throw new IllegalStateException(
        "First key in chunk already set: " + Bytes.toStringBinary(firstKeyInChunk));
    }
    // This will be done only once per chunk
    if (bloomType == BloomType.ROWCOL) {
      firstKeyInChunk = PrivateCellUtil
        .getCellKeySerializedAsKeyValueKey(PrivateCellUtil.createFirstOnRowCol(cell));
    } else {
      firstKeyInChunk = CellUtil.copyRow(cell);
    }
    allocateNewChunk();
  }

  chunk.add(cell);
  this.prevCell = cell;
  ++totalKeyCount;
}
```

可以看到关键操作是chunk.add(cell)，继续追踪

```java
// BloomFilterChunk.java
public void add(Cell cell) {
  /*
   * For faster hashing, use combinatorial generation
   * http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
   */
  int hash1;
  int hash2;
  HashKey<Cell> hashKey;
  if (this.bloomType == BloomType.ROWCOL) {
    hashKey = new RowColBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  } else {
    hashKey = new RowBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  }
  setHashLoc(hash1, hash2);
}
```

这个函数和里面的setHashLoc就包含了生成bloom的关键函数，能找到这里，原理也就看懂了。

但是思考一个问题，这个bloom只是add到了chunk中，但是最终我们肯定是要存到磁盘上的，那么chunk中的数据什么时候写入到磁盘的呢？

通过对StoreFileWriter的源码进行分析，我们可以知道在close方法中，bloom被写入到writer中，这里的writer实际上是一个HFileWriterImpl对象：

```java
private boolean closeGeneralBloomFilter() throws IOException {
  boolean hasGeneralBloom = closeBloomFilter(generalBloomFilterWriter);

  // add the general Bloom filter writer and append file info
  if (hasGeneralBloom) {
    writer.addGeneralBloomFilter(generalBloomFilterWriter);
    writer.appendFileInfo(BLOOM_FILTER_TYPE_KEY, Bytes.toBytes(bloomType.toString()));
    if (bloomParam != null) {
      writer.appendFileInfo(BLOOM_FILTER_PARAM_KEY, bloomParam);
    }
    bloomContext.addLastBloomKey(writer);
  }
  return hasGeneralBloom;
}
```

在HFileWriterImpl类中的close方法中，之前添加到wirter的additionalLoadOnOpenData（里面存着的就是bloom filter的wirter）写入磁盘

```java
for (BlockWritable w : additionalLoadOnOpenData) {
  blockWriter.writeBlock(w, outputStream);
  totalUncompressedBytes += blockWriter.getUncompressedSizeWithHeader();
}
// 还有其他很多信息落盘，这里都省略了
```



bloom读流程:

通过UT中的readStoreFile可以看出如何读取hfile中的block

```java
private void readStoreFile(Path storeFilePath, Configuration conf, ByteBuffAllocator alloc)
  throws Exception {
  // Open the file reader with block cache disabled.
  CacheConfig cache = new CacheConfig(conf, null, null, alloc);
  HFile.Reader reader = HFile.createReader(fs, storeFilePath, cache, true, conf);
  long offset = 0;
  while (offset < reader.getTrailer().getLoadOnOpenDataOffset()) {
    HFileBlock block = reader.readBlock(offset, -1, false, true, false, true, null, null);
    offset += block.getOnDiskSizeWithHeader();
    block.release(); // return back the ByteBuffer back to allocator.
  }
  reader.close();
}
```

这里我调试过了, reader.readBlock读出来的block包含各种类型，具体有哪几种类型的block，见BlockType.java



#### BloomFilterChunk

这个类是HBase实现bloom filter的核心类，这算是存储Bloom的比较底层的的类了，但不是最底层，最底层的是隶属于这个类的成员对象`protected ByteBuffer bloom`，也就是说bloom最底层是NIO数组。

但是BloomFilterChunk仍然是bloom最重要的类，接下来我讲讲理由

对于每一个BloomFilterChunk有：

成员函数：

keyCount：已经添加的key的数量

maxKeys：最大可以添加key的数量

那么这个最大数量是怎么来的呢？

```java
// BloomFilterUtil.java

/**
 * Creates a Bloom filter chunk of the given size.
 * @param byteSizeHint the desired number of bytes for the Bloom filter bit array. Will be
 *                     increased so that folding is possible.
 * @param errorRate    target false positive rate of the Bloom filter
 * @param hashType     Bloom filter hash function type
 * @return the new Bloom filter of the desired size
 */
public static BloomFilterChunk createBySize(int byteSizeHint, double errorRate, int hashType,
  int foldFactor, BloomType bloomType) {
  BloomFilterChunk bbf = new BloomFilterChunk(hashType, bloomType);

  bbf.byteSize = computeFoldableByteSize(byteSizeHint * 8L, foldFactor);
  long bitSize = bbf.byteSize * 8;
  bbf.maxKeys = (int) idealMaxKeys(bitSize, errorRate);
  bbf.hashCount = optimalFunctionCount(bbf.maxKeys, bitSize);

  // Adjust max keys to bring error rate closer to what was requested,
  // because byteSize was adjusted to allow for folding, and hashCount was
  // rounded.
  bbf.maxKeys = (int) computeMaxKeys(bitSize, errorRate, bbf.hashCount);

  return bbf;
}
```

从这个静态方法中我们可以看到，chunk的maxKeys其实是由 `bloom数组推荐的长度(byteSizeHint)`、`折叠率(foldFactor)`、`错误率(errorRate)`计算而来的。

我们继续，当HBase插入的key的数量大于bloom最大能存储的key的数量的时候，HBase不会进行扩容，而是会再新建一个chunk来接着存储。而之前已经存满的chunk会存入队列中。

```java
// CompoundBloomFilterWriter.java
@Override
public void append(Cell cell) throws IOException {
  Objects.requireNonNull(cell);

  enqueueReadyChunk(false);

  if (chunk == null) {
    if (firstKeyInChunk != null) {
      throw new IllegalStateException(
        "First key in chunk already set: " + Bytes.toStringBinary(firstKeyInChunk));
    }
    // This will be done only once per chunk
    if (bloomType == BloomType.ROWCOL) {
      firstKeyInChunk = PrivateCellUtil
        .getCellKeySerializedAsKeyValueKey(PrivateCellUtil.createFirstOnRowCol(cell));
    } else {
      firstKeyInChunk = CellUtil.copyRow(cell);
    }
    allocateNewChunk();
  }

  chunk.add(cell);
  this.prevCell = cell;
  ++totalKeyCount;
}
```

我们可以看到，在每次Bloom添加key前，会执行`enqueueReadyChunk(false)`，用处在于判断chunk是否存满，是否要新建一个chunk继续存储。

还有一点需要注意，存入队列之前，会对chunk进行压缩，在考虑Bloom的内存占用是需要考虑进来，具体见下面代码的注释

```java
// CompoundBloomFilterWriter.java
private void enqueueReadyChunk(boolean closing) {
  if (chunk == null || (chunk.getKeyCount() < chunk.getMaxKeys() && !closing)) {
    return;
  }

  if (firstKeyInChunk == null) {
    throw new NullPointerException(
      "Trying to enqueue a chunk, " + "but first key is null: closing=" + closing + ", keyCount="
        + chunk.getKeyCount() + ", maxKeys=" + chunk.getMaxKeys());
  }

  ReadyChunk readyChunk = new ReadyChunk();
  readyChunk.chunkId = numChunks - 1;
  readyChunk.chunk = chunk;
  readyChunk.firstKey = firstKeyInChunk;
  readyChunks.add(readyChunk);

  long prevMaxKeys = chunk.getMaxKeys();
  long prevByteSize = chunk.getByteSize();

  chunk.compactBloom(); // 内存压缩

  if (LOG.isTraceEnabled() && prevByteSize != chunk.getByteSize()) {
    LOG.trace("Compacted Bloom chunk #" + readyChunk.chunkId + " from [" + prevMaxKeys
      + " max keys, " + prevByteSize + " bytes] to [" + chunk.getMaxKeys() + " max keys, "
      + chunk.getByteSize() + " bytes]");
  }

  totalMaxKeys += chunk.getMaxKeys();
  totalByteSize += chunk.getByteSize();

  firstKeyInChunk = null;
  prevChunk = chunk;
  chunk = null;
}
```

明确了这一点，我们接着往下看，之前已经分析过chunk.add(cell)

```java
// BloomFilterChunk.java
public void add(Cell cell) {
  /*
   * For faster hashing, use combinatorial generation
   * http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
   */
  int hash1;
  int hash2;
  HashKey<Cell> hashKey;
  if (this.bloomType == BloomType.ROWCOL) {
    hashKey = new RowColBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  } else {
    hashKey = new RowBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  }
  setHashLoc(hash1, hash2);
}
```

现在我们看看细节，在setHashLoc之前这些hash类型是怎么实现的？



#### Hash Type

HBase中bloom实现有三种可替换的hash函数，分别为：

- Jenkins Hash
- Murmur Hash
- Murmur Hash3

默认是Murmur Hash, 可通过"hbase.hash.type"配置

可以通过以下源码得知：

```java
// Hash.java
public static int getHashType(Configuration conf) {
  String name = conf.get("hbase.hash.type", "murmur");
  return parseHashType(name);
}
```



那么我们可以得到HBase默认的hash函数murmur

```java
// MurmurHash.java
@Override
public <T> int hash(HashKey<T> hashKey, int seed) {
  int m = 0x5bd1e995;
  int r = 24;
  int length = hashKey.length();
  int h = seed ^ length;

  int len_4 = length >> 2;

  for (int i = 0; i < len_4; i++) {
    int i_4 = (i << 2);
    int k = hashKey.get(i_4 + 3);
    k = k << 8;
    k = k | (hashKey.get(i_4 + 2) & 0xff);
    k = k << 8;
    k = k | (hashKey.get(i_4 + 1) & 0xff);
    k = k << 8;
    // noinspection PointlessArithmeticExpression
    k = k | (hashKey.get(i_4 + 0) & 0xff);
    k *= m;
    k ^= k >>> r;
    k *= m;
    h *= m;
    h ^= k;
  }

  // avoid calculating modulo
  int len_m = len_4 << 2;
  int left = length - len_m;
  int i_m = len_m;

  if (left != 0) {
    if (left >= 3) {
      h ^= hashKey.get(i_m + 2) << 16;
    }
    if (left >= 2) {
      h ^= hashKey.get(i_m + 1) << 8;
    }
    if (left >= 1) {
      h ^= hashKey.get(i_m);
    }

    h *= m;
  }

  h ^= h >>> 13;
  h *= m;
  h ^= h >>> 15;

  return h;
}
```

至于我为什么要分析bloom的实现，原因在于我在预研ribbon filter去替换hbase中的bloom filter。为了证明ribbon filter确实比bloom filter更好，我需要使用c++重新实现bloom filter然后与rocksdb中的ribbon filter做对比测试。

至于为什么不用java实现ribbon filter，然后和hbase中的bloom做对比。当然是ribbon的实现太复杂啦。能调api，谁想去造轮子呢？

这部分内容请看 [预研](/大数据/2023/11/23/过滤器优化.html)



#### bloom创建时机测试

代码找到了，远程调试看看呢？

```shell
create "testbloom",{NAME=>'info0', BLOOMFILTER=>'ROW'}

put "testbloom","test_row1","info0:q1","testvalue1"
put "testbloom","test_row2","info0:q1","testvalue2"

scan "testbloom",COLUMN=>['info0:q1'],FILTER=>"ValueFilter(=,'substring:test')"
get "testbloom","testrow1",{COLUMNS=>['info0:q1']}
```

在单机集群上（只存在一个HMaster和一个HRegionServer），同时连接master和regionserver，执行上面的命令，发现并没有走到创建和读取bloom的逻辑当中。。

##### 下面找找什么情况下会进入bloom的生成逻辑？

貌似都是自动触发的flush

```
当前疑点：

1、为什么Master和RegionServer都要定时刷新？

解答：Master定时刷新 MemStore 而RegionServer定时刷新（？）
```

1、hbase pe 随机写（怀疑是WAL达到一定值，自动进行了flush）

```shell
hbase pe --nomapred --oneCon=true --table=testbloom --size=100 --valueSize=48 --columns=16 sequentialWrite 16

```

2、放置一会，什么都不干会触发Master的flush，从而一步一步走到bloom的生成逻辑。（有个flushQueue，循环判断Queue是否有成员，如果有，则自动进行MemStore的flush）

```java
// MasterRegionFlusherAndCompactor.java
private void flushLoop() {
  recordLastFlushTime();
  while (!closed) {
    flushLock.lock();
    try {
      while (!flushRequest) { // 从这里可以看出每隔flushIntervalMs就会刷新一次Region
        long waitTimeMs = lastFlushTime + flushIntervalMs - EnvironmentEdgeManager.currentTime();
        if (waitTimeMs <= 0) {
          flushRequest = true;
          break;
        }
        flushCond.await(waitTimeMs, TimeUnit.MILLISECONDS);
        if (closed) {
          return;
        }
      }
    } catch (InterruptedException e) {
      Thread.currentThread().interrupt();
      continue;
    } finally {
      flushLock.unlock();
    }
    assert flushRequest;
    resetChangesAfterLastFlush();
    try {
      region.flush(true); // 这里就是关键的刷新调用了
      recordLastFlushTime();
    } catch (IOException e) {
      LOG.error(HBaseMarkers.FATAL, "Failed to flush master local region, aborting...", e);
      abortable.abort("Failed to flush master local region", e);
      return;
    }
    compactLock.lock();
    try {
      if (!compactRequest && needCompaction()) {
        compactRequest = true;
        compactExecutor.execute(this::compact);
      }
    } finally {
      compactLock.unlock();
    }
    flushLock.lock();
    try {
      // reset the flushRequest flag
      if (!shouldFlush(changesAfterLastFlush.get())) {
        flushRequest = false;
      }
    } finally {
      flushLock.unlock();
    }
  }
}
```

在MasterRegionFlusherAndCompactor的构造函数中，创建了一个线程，定时刷新Region

```java
flushThread = new Thread(this::flushLoop, region.getRegionInfo().getTable() + "-Flusher");
```

你可能会有疑问，刷新Region和创建bloom有什么关系，下面简单解释一下：

首先关键调用`region.flush(true);`最终会调到flushcache方法

```java
//  public FlushResultImpl flushcache(List<byte[]> families, boolean writeFlushRequestWalMarker,
//  FlushLifeCycleTracker tracker) throws IOException
try {
    // The reason that we do not always use flushPolicy is, when the flush is
    // caused by logRoller, we should select stores which must be flushed
    // rather than could be flushed.
    Collection<HStore> specificStoresToFlush = null;
    if (families != null) {
      specificStoresToFlush = getSpecificStores(families);
    } else {
      specificStoresToFlush = flushPolicy.selectStoresToFlush();
    }
    FlushResultImpl fs =
      internalFlushcache(specificStoresToFlush, status, writeFlushRequestWalMarker, tracker);
。。。
}

private Collection<HStore> getSpecificStores(List<byte[]> families) {
  Collection<HStore> specificStoresToFlush = new ArrayList<>();
  for (byte[] family : families) {
    specificStoresToFlush.add(stores.get(family)); //  根据family取出对应的Store
  }
  return specificStoresToFlush;
}
```

getSpecificStores会按照families将对应的HStore取出来，看下getSpecificStores的实现，可以得出结论：一个family对应一个HStore

接着调用internalFlushcache方法进入内部刷新实现，`Master实际刷新的是MemStoreSnapshot`

```java
// HStore.java
protected List<Path> flushCache(final long logCacheFlushId, MemStoreSnapshot snapshot,
  MonitoredTask status, ThroughputController throughputController, FlushLifeCycleTracker tracker,
  Consumer<Path> writerCreationTracker) throws IOException {
  // If an exception happens flushing, we let it out without clearing
  // the memstore snapshot. The old snapshot will be returned when we say
  // 'snapshot', the next time flush comes around.
  // Retry after catching exception when flushing, otherwise server will abort
  // itself
  StoreFlusher flusher = storeEngine.getStoreFlusher();
  IOException lastException = null;
  for (int i = 0; i < flushRetriesNumber; i++) {
    try {
      List<Path> pathNames = flusher.flushSnapshot(snapshot, logCacheFlushId, status,
        throughputController, tracker, writerCreationTracker); // 刷新memstore snapshot
      Path lastPathName = null;
....
```



3、放置一会，什么都不干会触发HRegion的flush，从而一步一步走到bloom的生成逻辑。

源头在MemStoreFlusher的flushHandlers，他存储了多个flush线程，用于刷新MemStore中的排队的Region，来源于flushQueue



##### 下面找找什么情况下会进入bloom的读取逻辑？

之前找的地方不对，读取bloom逻辑不在chunk里面，而被放在了BloomFilterUtil中

```java
// BloomFilterUtil.java
static boolean checkBit(int pos, ByteBuff bloomBuf, int bloomOffset) {
  int bytePos = pos >> 3; // pos / 8
  int bitPos = pos & 0x7; // pos % 8
  byte curByte = bloomBuf.get(bloomOffset + bytePos);
  curByte &= bitvals[bitPos];
  return (curByte != 0);
}
```

使用下面的测试用例可以直接走到bloom读取

```shell
create "testbloom",{NAME=>'info0', BLOOMFILTER=>'ROW'}

put "testbloom","test_row1","info0:q1","testvalue1"
put "testbloom","test_row2","info0:q1","testvalue2"

scan "testbloom",COLUMN=>['info0:q1'],FILTER=>"ValueFilter(=,'substring:test')"
get "testbloom","testrow1",{COLUMNS=>['info0:q1']}
```

下面大致描述一下读流程：

- Client读取“hbase:meta”,找到对应的HRegion，以及他在哪一台机器上，然后发RPC请求过去

- RegionServer接受Rpc请求，通过getScanner, 获取scanner给client，下面就是获取Scanner的过程

  ```java
  // HRegion.java
  @Override
  public RegionScannerImpl getScanner(Scan scan) throws IOException {
    return getScanner(scan, null);
  }
  ```

- 跳转，调用 `instantiateRegionScanner(scan, additionalScanners, nonceGroup, nonce);`这个方法返回一个RegionScannerImpl,这就是我们想要的

- 在RegionScannerImpl的构造函数最后，调用`initializeScanners(scan, additionalScanners);`

  ```java
  private void initializeScanners(Scan scan, List<KeyValueScanner> additionalScanners) {
    。。。
    try {
      for (Map.Entry<byte[], NavigableSet<byte[]>> entry : scan.getFamilyMap().entrySet()) {
        HStore store = region.getStore(entry.getKey());
        KeyValueScanner scanner = store.getScanner(scan, entry.getValue(), this.readPt); // 这里
        instantiatedScanners.add(scanner);
  	。。。
  }
  ```

- 在store.getScanner中，返回StoreScanner对象，该对象的构造函数对Scanner进行过滤

  ```java
  // StoreScanner.java
  public StoreScanner(HStore store, ScanInfo scanInfo, Scan scan, NavigableSet<byte[]> columns,
    long readPt) throws IOException {
    this(store, scan, scanInfo, columns != null ? columns.size() : 0, readPt, scan.getCacheBlocks(),
      ScanType.USER_SCAN);
    if (columns != null && scan.isRaw()) {
      throw new DoNotRetryIOException("Cannot specify any column for a raw scan");
    }
    matcher = UserScanQueryMatcher.create(scan, scanInfo, columns, oldestUnexpiredTS, now,
      store.getCoprocessorHost());
  
    store.addChangedReaderObserver(this);
  
    List<KeyValueScanner> scanners = null;
    try {
      // Pass columns to try to filter out unnecessary StoreFiles.
      scanners = selectScannersFrom(store,
        store.getScanners(cacheBlocks, scanUsePread, false, matcher, scan.getStartRow(),
          scan.includeStartRow(), scan.getStopRow(), scan.includeStopRow(), this.readPt)); // 这里
  	...
  }
  ```

- selectScannersFrom就是根据scan对隶属于store的所有scanner做过滤，具体看shouldUseScanner，在该方法最后对scanner做了

  - 1、时间范围过滤
  - 2、key范围过滤
  - 3、bloom过滤

  ```java
  // StoreScanner.java
  protected List<KeyValueScanner> selectScannersFrom(HStore store, ... ) {
    。。。
    // include only those scan files which pass all filters
    for (KeyValueScanner kvs : allScanners) {
      boolean isFile = kvs.isFileScanner();
      if ((!isFile && filesOnly) || (isFile && memOnly)) {
        kvs.close();
        continue;
      }
  
      if (kvs.shouldUseScanner(scan, store, expiredTimestampCutoff)) { // shouldUseScanner
        scanners.add(kvs);
      } else {
        kvs.close();
      }
    }
    。。。                                                  
  }      
  
  // StoreFileScanner.java
  @Override
  public boolean shouldUseScanner(Scan scan, HStore store, long oldestUnexpiredTS) {
    // if the file has no entries, no need to validate or create a scanner.
    byte[] cf = store.getColumnFamilyDescriptor().getName();
    TimeRange timeRange = scan.getColumnFamilyTimeRange().get(cf);
    if (timeRange == null) {
      timeRange = scan.getTimeRange();
    }
    return reader.passesTimerangeFilter(timeRange, oldestUnexpiredTS)
      && reader.passesKeyRangeFilter(scan)
      && reader.passesBloomFilter(scan, scan.getFamilyMap().get(cf));
  }
  ```

  

- 上面的reader是一个StoreFileRead对象（写bloom的时候是StoreFlileWrite对象），后面就是以passesBloomFilter开头最后调用到BloomFilterUtil.java的checkBit，难度较低，不写了。这就是调用bloom过滤的完整流程。

  
