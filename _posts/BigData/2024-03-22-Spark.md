---
layout: page-with-sidebar
title:  "Spark"
date:   2024-03-22 14:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
hideTag: false
---

## 安装部署问题

### 1、Spark Web UI访问不了

Spark Web UI可以看到历史执行的sql以及对应的执行计划、metrics等，对于开发很重要。假设Web UI的端口为18080，有时候我们无法访问他：

1、检查代理，在公司内网上外网需要代理，但是往往Spark部署在内网集群上，内网机器是不需要代理的。这时我们在windows上输入“代理”，打开代理服务器设置。在`请勿对以下列条目开头的地址使用代理服务器。。。`后面加上内网机器的IP。

2、检查集群里的HistoryServer有没有启动：

HistoryServer这个进程就是Web UI的服务进程，如果没有启动，是肯定访问不了web ui的。

配置history server：主要分两部分

- 第一部分：配置spark-defaults.conf

  ```conf
  spark.eventLog.enabled   true
  spark.eventLog.dir       hdfs://node7-2:8020/directory
  ```

  这个设置表示打开spark的日志系统，并设置日志输出目录。（没有日志输出，web ui也是用不了，web ui本质上是把磁盘的log文件可视化）

- 第二部分：设置spark-env.sh

  ```bash
  
   export SPARK_HISTORY_OPTS="
  -Dspark.history.ui.port=18080
  -Dspark.history.retainedApplications=30
  -Dspark.history.fs.logDirectory=hdfs://node7-2:8020/directory"
  ```

  这一部分设置了history server要去哪里找spark的日志（和上面的日志输出路径保持一致即可）

配置结束了，现在去spark/sbin目录下，运行 start-history-server.sh 即可拉起web ui服务。

[参考资料](https://blog.csdn.net/XIAOMO__/article/details/107988164)



## 源码特性

### 1、whole stage code gen

Spark中的Whole-Stage Code Generation（整阶段代码生成）是一种优化技术，用于提高Spark SQL查询的性能。它的主要思想是将一整个查询阶段的代码生成为一个单独的函数，以减少函数调用和数据序列化/反序列化的开销。

在Spark中，查询通常由多个操作组成，例如选择、过滤、聚合等。每个操作都可以表示为一个函数，但这些函数通常会导致较多的函数调用开销和数据的序列化/反序列化开销。为了优化这一点，Spark使用Whole-Stage Code Generation将多个操作合并为一个函数，并生成高效的Java字节码来执行这个函数。

整阶段代码生成的过程大致如下：

1. Spark SQL将逻辑查询计划转换为物理查询计划。
2. 物理查询计划被划分为一系列称为“物理子计划”的阶段，每个子计划包含一组可以一起执行的操作。
3. 对于每个物理子计划，Spark生成一个包含所有操作的单个函数，并为该函数生成优化的Java字节码。
4. 在运行时，生成的函数被传递给Spark执行引擎执行，从而减少了函数调用和数据的序列化/反序列化开销。

整阶段代码生成可以大大提高Spark SQL查询的性能，特别是在涉及大规模数据处理时。通过减少函数调用和数据的序列化/反序列化开销，整阶段代码生成可以显著提升查询的执行速度。

### 2、cloumnar

在Spark中，"columnar" 通常用于描述一种数据存储和处理的方式，即按列存储。与传统的按行存储相比，按列存储将同一列的数据连续存储在一起，而不是将整行数据连续存储。这种存储方式在数据分析和处理中具有一些优势，特别是对于大规模数据集。

在 Spark SQL 中，列式存储通常指的是 Parquet 格式。Parquet 是一种列式存储格式，它将数据按列存储在文件中，这样做有几个好处：

1. **高压缩率**：相同的值在列中是连续的，这样可以更容易地实现高效的压缩，减少存储空间的使用。
2. **只读查询效率高**：当查询只需要访问部分列时，只需要读取这些列的数据，而不必读取整行数据，这可以减少读取的数据量，提高查询效率。
3. **谓词下推**：列式存储使得更容易进行谓词下推，即在数据存储时就可以排除不满足条件的数据，减少查询时需要处理的数据量。

Spark SQL 在处理 Parquet 格式的数据时，会利用列式存储的优势来提高查询性能和降低资源消耗。例如，当使用整阶段代码生成来执行查询时，Spark 可以有效地处理列式存储的数据，从而加速查询的执行。

总的来说，列式存储在 Spark 中被广泛应用于大规模数据处理场景，可以提供更高效的数据存储和处理方式，特别是对于分析型工作负载。

### 3、AQE

在Apache Spark中，AQE（Adaptive Query Execution，自适应查询执行）是一种优化技术，旨在根据作业的运行情况动态地调整执行计划，以提高性能并降低资源消耗。AQE可以在运行时根据数据的分布、任务的执行情况和资源可用性等因素进行优化，从而更好地适应不同的工作负载和环境。

AQE的主要特点和功能包括：

1. **动态优化执行计划**：根据作业的运行情况，AQE可以动态地调整执行计划，例如更改连接策略、重新分区或更改过滤器等，以提高查询性能。
2. **自适应分区**：根据数据分布和操作的需要，AQE可以自动调整数据的分区方式，以减少数据倾斜并提高并行性。
3. **动态过滤器推断**：AQE可以根据运行时数据的统计信息推断出更好的过滤器，以减少需要处理的数据量。
4. **自适应扫描**：根据数据分布和过滤器条件，AQE可以选择合适的扫描方式，例如跳过不需要的数据块或者选择更有效的扫描方式。
5. **动态重分区**：根据数据倾斜和任务执行情况，AQE可以动态地重新分区数据，以平衡任务负载并提高并行性能。

通过这些功能，AQE可以在不同的查询场景下自动调整执行计划，从而提高查询性能和资源利用率。AQE是Spark 3.0中引入的功能，并在后续版本中不断改进和优化，成为了Spark SQL中重要的优化技术之一。

### 4、Origin

Orgin用于存储行号、sql文本、起止位置等

CurrentOrigin中有一个value变量，本质是一个LocalThread[Origin]

常看到下面的代码：

```scala
CurrentOrigin.withOrigin(origin) {
	 要执行的操作
}
// withOrigin做的事情是，如果“要执行的操作”正常执行（没有报错，没有try到异常），则设置新的orgin，否则恢复原来的origin。
```

`CurrentOrigin.withOrigin` 是 Spark 中的一个方法，用于在当前线程上设置一个新的 SQL 分析上下文（`SQLContext`）的 origin。在 Spark 中，origin 用于跟踪和诊断 SQL 查询或操作的来源，以便在日志或错误消息中提供更多的上下文信息。

该方法的签名通常是这样的：

```scala
def withOrigin[T](origin: Origin)(f: => T): T
```

其中，`origin` 是一个表示查询或操作来源的对象，通常包含文件名、行号等信息。`f` 是一个函数，表示需要在设置了新的 origin 的上下文中执行的代码块。该方法会返回 `f` 的结果。

使用 `CurrentOrigin.withOrigin` 方法可以在 Spark 中为特定的操作设置 origin，以便在日志中能够更容易地追踪和诊断问题。





