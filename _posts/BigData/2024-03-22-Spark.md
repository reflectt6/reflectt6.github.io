---
layout: page-with-sidebar
title:  "Spark"
date:   2024-03-22 14:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
hideTag: false
---

## 问答

### 1、hive和spark sql 的区别？

Hive 和 Spark SQL 都是用于大数据处理的工具，它们在架构、执行引擎、性能和使用场景等方面有一些重要的区别。

#### 架构和执行引擎

**Hive**:

- **架构**：Hive 基于 MapReduce 架构，最初是为了解决大规模数据仓库的查询问题。
- **执行引擎**：默认使用 Hadoop 的 MapReduce 作为执行引擎，但也支持其他引擎如 Tez 和 Spark。
- **执行模式**：批处理模式。每个查询都会被转化为一系列的 MapReduce 任务，这些任务在 Hadoop 集群上运行。

**Spark SQL**:

- **架构**：Spark SQL 是 Apache Spark 的一部分，它允许使用 SQL 语句来处理数据，并且与 Spark 的核心 API 紧密集成。
- **执行引擎**：基于 Spark 的 DAG (Directed Acyclic Graph) 计算引擎。
- **执行模式**：支持批处理和实时流处理（通过 Spark Streaming），并且查询被转化为 Spark 的内部 RDD 操作，执行效率更高。

#### 性能

**Hive**（基于磁盘）:

- **性能**：由于 MapReduce 的批处理特性，Hive 的查询性能通常较慢，特别是对于小型查询和交互式查询。MapReduce 的启动开销较大，并且每个查询的执行需要多个阶段，导致延迟较高。

**Spark SQL**（基于内存）:

- **性能**：由于 Spark 的内存计算特性，Spark SQL 的性能通常比 Hive 高很多。Spark 可以将数据缓存到内存中，从而减少 I/O 操作，显著提高查询速度。其基于 DAG 的优化器（Catalyst）和内存计算模型使其在处理迭代计算和复杂查询时具有明显的优势。

#### 使用场景

**Hive**:

- **使用场景**：主要用于离线批处理数据分析，适合处理大规模数据集，并生成定期报告和数据分析结果。由于 Hive 使用 SQL 语法，适合那些熟悉 SQL 的用户和数据分析师。

**Spark SQL**:

- **使用场景**：适用于需要高性能、低延迟的交互式查询和实时数据分析的场景。它还可以用于批处理数据分析，流处理，以及机器学习等多种应用场景。Spark SQL 与 Spark 的其他组件（如 Spark Streaming、MLlib）无缝集成，使其成为一个功能全面的数据处理平台。

4. 数据存储和兼容性

**Hive**:

- **数据存储**：Hive 支持多种存储格式，如文本文件、SequenceFile、ORC、Parquet 等。数据通常存储在 HDFS 中。
- **兼容性**：Hive 的表结构存储在 Hive Metastore 中，支持与 Hadoop 生态系统中的其他组件（如 Pig、HBase）的集成。

**Spark SQL**:

- **数据存储**：Spark SQL 也支持多种存储格式，如 JSON、Parquet、ORC、Avro 等。数据可以存储在 HDFS、S3、HBase、Cassandra 等多种存储系统中。
- **兼容性**：Spark SQL 具有高度的灵活性，可以与多种数据源和存储系统集成。它还提供了 JDBC/ODBC 接口，使其可以与传统的 BI 工具（如 Tableau、Power BI）集成。兼容性：Spark SQL 具有高度的灵活性，可以与多种数据源和存储系统集成。它还提供了 JDBC/ODBC 接口，使其可以与传统的 BI 工具（如 Tableau、Power BI）集成。

#### 总结

- **Hive** 适用于大规模批处理数据分析，通常用于数据仓库场景，查询延迟较高。
- **Spark SQL** 适用于需要快速响应和实时处理的场景，具有更高的性能和灵活性，适合广泛的数据处理和分析任务。



### 2、什么是批处理和流处理？

批处理和流处理是两种不同的数据处理模式，适用于不同类型的应用和数据需求。

#### 批处理 (Batch Processing)

批处理是一种一次性处理大量数据的模式，通常用于定期执行的任务，比如每天或每小时运行的作业。批处理系统会在固定的时间间隔内收集数据，然后对这些数据进行处理。处理完成后，会输出结果。

**特点**：

- **延迟较高**：由于需要收集一批数据后再进行处理，处理和结果输出之间存在时间延迟。
- **高吞吐量**：适合处理大规模的数据集，因为可以一次性处理大量数据。
- **常见场景**：数据仓库更新、日志分析、报表生成、大数据处理等。

**常用工具**：

- **Hadoop MapReduce**：经典的批处理框架，用于处理和生成大规模数据集。
- **Apache Hive**：基于 Hadoop 的数据仓库系统，可以使用 SQL 查询来进行批处理数据分析。
- **Apache Spark**：尽管 Spark 也支持流处理，但其最初设计就是为了高效进行批处理。

**示例**：

```
scala
复制代码
val conf = new SparkConf().setAppName("Batch Processing Example")
val sc = new SparkContext(conf)
val data = sc.textFile("hdfs://path/to/batch/data")
val result = data.flatMap(line => line.split(" "))
                .map(word => (word, 1))
                .reduceByKey(_ + _)
result.saveAsTextFile("hdfs://path/to/output")
```

#### 流处理 (Stream Processing)

流处理是一种实时处理数据流的模式，数据会在产生的同时被处理，这样可以实时地生成处理结果。流处理系统会持续接收和处理数据流，而不是等待数据收集完成后再进行处理。

**特点**：

- **低延迟**：数据在产生后立即被处理，能够实时生成结果。
- **实时性强**：适合需要即时响应的应用，比如实时监控、在线分析、金融交易等。
- **常见场景**：实时数据分析、事件检测、实时监控、在线推荐系统等。

**常用工具**：

- **Apache Kafka**：消息队列系统，常用于流数据的传输和缓冲。
- **Apache Flink**：专门为流处理设计的框架，提供低延迟、高吞吐量的实时处理能力。
- **Apache Spark Streaming**：Spark 的流处理扩展，支持将流数据处理集成到 Spark 的统一编程模型中。

**示例**：

```scala
val conf = new SparkConf().setAppName("Stream Processing Example")
val ssc = new StreamingContext(conf, Seconds(1))
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
```

#### 总结

- **批处理**：适合处理大规模、非实时的数据，通常用于定期执行的任务，处理延迟较高，但吞吐量大。
- **流处理**：适合需要实时处理和即时响应的应用，能够以低延迟处理数据流，适合实时监控和分析。流处理：适合需要实时处理和即时响应的应用，能够以低延迟处理数据流，适合实时监控和分析。

这两种模式可以根据具体的应用需求进行选择，有时也可以结合使用，以满足不同的数据处理需求。

### 3、Spark中的宽依赖、窄依赖？

在 Apache Spark 中，宽依赖（Wide Dependency）和窄依赖（Narrow Dependency）是指不同类型的 RDD（Resilient Distributed Dataset）之间的依赖关系。这两种依赖关系影响了 Spark 作业的执行方式、性能和容错机制。

#### 窄依赖 (Narrow Dependency)

窄依赖是指每个父 RDD 的分区最多被一个子 RDD 的分区使用，即父 RDD 的分区和子 RDD 的分区之间存在一对一的映射关系。这种依赖关系通常发生在诸如 `map`、`filter` 等操作中。

**特点**：

- **局部性好**：由于父 RDD 的数据只会传递到一个子 RDD 的分区，数据可以在同一个节点上处理，减少了数据的传输开销。
- **容易恢复**：在容错机制中，如果某个分区数据丢失，只需要重新计算对应的父 RDD 的分区即可，恢复过程简单高效。

**示例**：

```
scala
复制代码
val rdd1 = sc.parallelize(Seq(1, 2, 3, 4))
val rdd2 = rdd1.map(_ * 2)  // 窄依赖，每个 rdd1 的分区只被一个 rdd2 的分区使用
```

#### 宽依赖 (Wide Dependency)

宽依赖是指一个父 RDD 的分区可能被多个子 RDD 的分区使用，即父 RDD 的分区和子 RDD 的分区之间存在多对多的映射关系。这种依赖关系通常发生在诸如 `reduceByKey`、`groupByKey` 等需要重新分区（shuffle）的操作中。

**特点**：

- **需要 shuffle**：宽依赖会触发数据的重新分区，需要通过网络传输数据，开销较大。
- **复杂恢复**：在容错机制中，如果某个分区数据丢失，需要重新计算多个父 RDD 的分区，恢复过程复杂且耗时。

**示例**：

```scala
val rdd1 = sc.parallelize(Seq((1, 2), (3, 4), (3, 6)))
val rdd2 = rdd1.reduceByKey(_ + _)  // 宽依赖，rdd1 的多个分区会被 rdd2 的分区使用
```

#### 影响

1. **执行性能**：
   - 窄依赖的操作通常比宽依赖的操作更高效，因为它们可以在单个节点上完成，避免了数据的网络传输。
   - 宽依赖操作会触发数据的 shuffle，增加了执行时间和资源消耗。
2. **容错性**：
   - 窄依赖的操作在容错时更简单，只需要重新计算丢失的分区。
   - 宽依赖的操作在容错时较为复杂，因为需要重新计算多个分区。宽依赖的操作在容错时较为复杂，因为需要重新计算多个分区。
3. **调度和优化**：
   - Spark 的执行计划（DAG）会根据依赖关系进行优化，窄依赖的任务会尽量在本地执行，而宽依赖的任务需要考虑数据的重新分区和传输。

#### 总结

理解宽依赖和窄依赖对于优化 Spark 作业的性能和理解其执行模型非常重要。在实际应用中，应尽量减少宽依赖操作的使用，或者将它们放在必要时使用，以减少 shuffle 操作，提升整体的执行效率。



### 4、DAG、有向无环图

找最短路径（代价最低的路径）

有向无环图（Directed Acyclic Graph，简称 DAG）是图论中的一种特殊结构，它具有以下特点：

1. **有向**：图中的边有方向，即每条边从一个节点指向另一个节点。
2. **无环**：图中不存在任何从一个节点出发并返回该节点的路径（即不存在环）。

DAG 在计算机科学、数据处理、任务调度等领域有广泛的应用。例如，在 Apache Spark 中，DAG 用于表示计算任务的依赖关系和执行顺序。

#### DAG 的性质

- **拓扑排序**：DAG 可以进行拓扑排序，即将所有节点排列成一个线性序列，使得对于每一条有向边 (u, v)，节点 u 在节点 v 之前。拓扑排序是 DAG 的一个重要性质，可以用于任务调度和依赖管理。
- **最长路径问题**：在有环图中，寻找最长路径问题是 NP 完全问题，但在 DAG 中可以在线性时间内解决。
- **最短路径问题**：在 DAG 中可以通过拓扑排序高效地解决最短路径问题，特别是当边权重为正时。

#### DAG 的应用

1. **任务调度**：DAG 常用于表示任务之间的依赖关系，确保任务按照正确的顺序执行。例如，在编译器中，DAG 用于表示源代码文件之间的依赖关系，以确定编译顺序。
2. **数据处理**：如 Apache Spark 中的 RDD 依赖关系图。在 Spark 中，计算任务被表示为一系列相互依赖的 RDD 操作，DAG 用于表示这些依赖关系和执行顺序，以优化和调度计算任务。
3. **版本控制系统**：如 Git，使用 DAG 来表示提交（commit）之间的关系。每个提交是一个节点，边表示提交之间的继承关系，确保没有循环依赖。
4. **构建系统**：如 Makefile 中的目标和依赖关系，通过 DAG 表示文件和生成目标之间的依赖，确保目标按照正确顺序构建。

#### DAG 示例

以下是一个简单的 DAG 示例，其中节点表示任务，边表示任务之间的依赖关系：

```
A → B → D
 \   \
  \   → E
   \
    → C → F
```

在这个示例中：

- 任务 A 是 B、C 和 E 的先决条件。
- 任务 B 和 C 必须在任务 D 和 F 之前完成。
- 任务 E 依赖于任务 B。

#### 拓扑排序示例

根据上面的 DAG，可能的拓扑排序有多种，例如：

1. A, B, C, D, E, F
2. A, C, B, E, D, F

DAG 是一种灵活且功能强大的数据结构，广泛应用于各种需要处理依赖关系和执行顺序的场景中。



## 安装部署

Spark有几种集群部署模式，具体请看[Cluster Mode Overview](https://spark.apache.org/docs/latest/cluster-overview.html){:target="_blank"}

比较关键的组件有Driver和Worker，见下图

![image-20240422143309188](/assets/images/2024-03-22-Spark//image-20240422143309188.png)

集中部署模式的区别在于Cluster Manager的不同

### Cluster Manager Types

The system currently supports several cluster managers:
系统目前支持多种集群管理器：

- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html){:target="_blank"} – a simple cluster manager included with Spark that makes it easy to set up a cluster.
  Standalone——Spark 附带的一个简单的集群管理器，可以轻松设置集群。

  要使用启动脚本启动 Spark 独立集群，您应该在 Spark 目录中创建一个名为 conf/workers 的文件，该文件必须包含您打算启动 Spark Worker 的所有计算机的主机名，每行一个。如果conf/workers不存在，启动脚本默认为单台机器（localhost），这对于测试很有用。请注意，主机通过 ssh 访问每台工作机。默认情况下，ssh 并行运行，并且需要设置无密码（使用私钥）访问。如果您没有无密码设置，则可以设置环境变量 SPARK_SSH_FOREGROUND 并连续为每个工作人员提供密码。

- [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html){:target="_blank"} – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)
  Apache Mesos – 通用集群管理器，还可以运行 Hadoop MapReduce 和服务应用程序。 （已弃用）

- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html){:target="_blank"} – the resource manager in Hadoop 3.
  Hadoop YARN – Hadoop 3 中的资源管理器。

- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html){:target="_blank"} – an open-source system for automating deployment, scaling, and management of containerized applications.
  Kubernetes – 一个用于自动化部署、扩展和管理容器化应用程序的开源系统。

### [Submitting Applications](https://spark.apache.org/docs/latest/submitting-applications.html){:target="_blank"}

Spark通过shell脚本提交任务，使用脚本参数可以随意设置任务运行在哪一种部署模式下，常见参数如下：

- `--class`: The entry point for your application (e.g. `org.apache.spark.examples.SparkPi`)
  `--class` ：应用程序的入口点（例如 `org.apache.spark.examples.SparkPi` ）

- `--master`: The [master URL](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls){:target="_blank"} for the cluster (e.g. `spark://23.195.26.187:7077`)

  Master URLs有哪些呢？如下表

  The master URL passed to Spark can be in one of the following formats:

  | Master URL                        | Meaning                                                      |
  | --------------------------------- | ------------------------------------------------------------ |
  | `local`                           | Run Spark locally with one worker thread (i.e. no parallelism at all). |
  | `local[K]`                        | Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). |
  | `local[K,F]`                      | Run Spark locally with K worker threads and F maxFailures (see [spark.task.maxFailures](https://spark.apache.org/docs/latest/configuration.html#scheduling){:target="_blank"} for an explanation of this variable). |
  | `local[*]`                        | Run Spark locally with as many worker threads as logical cores on your machine. |
  | `local[*,F]`                      | Run Spark locally with as many worker threads as logical cores on your machine and F maxFailures. |
  | `local-cluster[N,C,M]`            | Local-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker. |
  | `spark://HOST:PORT`               | Connect to the given [Spark standalone cluster](https://spark.apache.org/docs/latest/spark-standalone.html){:target="_blank"} master. The port must be whichever one your master is configured to use, which is 7077 by default. |
  | `spark://HOST1:PORT1,HOST2:PORT2` | Connect to the given [Spark standalone cluster with standby masters with Zookeeper](https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper){:target="_blank"}. The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default. |
  | `mesos://HOST:PORT`               | Connect to the given [Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html){:target="_blank"} cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use `mesos://zk://...`. To submit with `--deploy-mode cluster`, the HOST:PORT should be configured to connect to the [MesosClusterDispatcher](https://spark.apache.org/docs/latest/running-on-mesos.html#cluster-mode){:target="_blank"}. |
  | `yarn`                            | Connect to a [YARN ](https://spark.apache.org/docs/latest/running-on-yarn.html){:target="_blank"}cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The cluster location will be found based on the `HADOOP_CONF_DIR` or `YARN_CONF_DIR` variable. |
  | `k8s://HOST:PORT`                 | Connect to a [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html){:target="_blank"} cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The `HOST` and `PORT` refer to the [Kubernetes API Server](https://kubernetes.io/docs/reference/generated/kube-apiserver/){:target="_blank"}. It connects using TLS by default. In order to force it to use an unsecured connection, you can use `k8s://http://HOST:PORT`. |

- `--deploy-mode`: Whether to deploy your driver on the worker nodes (`cluster`) or locally as an external client (`client`) (default: `client`) **†**
  `--deploy-mode` ：是将驱动程序部署在工作节点 ( `cluster` ) 上还是作为外部客户端部署在本地 ( `client` )（默认值： `client` ) †

- `--conf`: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. `--conf <key>=<value> --conf <key2>=<value2>`)
  `--conf` ：任意 Spark 配置属性，采用 key=value 格式。对于包含空格的值，请将“key=value”括在引号中（如图所示）。多个配置应作为单独的参数传递。 （例如 `--conf <key>=<value> --conf <key2>=<value2>` ）

- `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.
  `application-jar` ：捆绑 jar 的路径，包括您的应用程序和所有依赖项。 URL 必须在集群内部全局可见，例如，所有节点上都存在的 `hdfs://` 路径或 `file://` 路径。

- `application-arguments`: Arguments passed to the main method of your main class, if any
  `application-arguments` ：传递给主类的 main 方法的参数（如果有）
  
- --conf spark.driver.extraClassPath=/opt/
  
  driver端额外需要添加的Java包
  
- --conf spark.executor.extraClassPath=/opt/

  executor端额外需要添加的Java包

- --conf spark.driverEnv.LD_LIBRARY_PATH=/opt/

  driver端额外需要添加的C++库

- --conf spark.executorEnv.LD_LIBRARY_PATH=/opt/

  Executor端额外需要添加的C++库

- --conf spark.driverEnv.OMNI_HOME=/opt/

  driver端额外需要添加的C++库路径 “OMNI_HOME”是自定义的环境变量名，用于c++代码加载自定义类库

- --conf spark.executorEnv.OMNI_HOME=/opt/

  Executor端额外需要添加的C++库路径 “OMNI_HOME”是自定义的环境变量名，用于c++代码加载自定义类库


## 常见部署问题

### 1、Spark Web UI访问不了

Spark Web UI可以看到历史执行的sql以及对应的执行计划、metrics等，对于开发很重要。假设Web UI的端口为18080，有时候我们无法访问他：

1、检查代理，在公司内网上外网需要代理，但是往往Spark部署在内网集群上，内网机器是不需要代理的。这时我们在windows上输入“代理”，打开代理服务器设置。在`请勿对以下列条目开头的地址使用代理服务器。。。`后面加上内网机器的IP。

2、检查集群里的HistoryServer有没有启动：

HistoryServer这个进程就是Web UI的服务进程，如果没有启动，是肯定访问不了web ui的。

配置history server：主要分两部分

- 第一部分：配置spark-defaults.conf

  ```conf
  spark.eventLog.enabled   true
  spark.eventLog.dir       hdfs://node7-2:8020/directory
  ```

  这个设置表示打开spark的日志系统，并设置日志输出目录。（没有日志输出，web ui也是用不了，web ui本质上是把磁盘的log文件可视化）

- 第二部分：设置spark-env.sh

  ```bash
  
   export SPARK_HISTORY_OPTS="
  -Dspark.history.ui.port=18080
  -Dspark.history.retainedApplications=30
  -Dspark.history.fs.logDirectory=hdfs://node7-2:8020/directory"
  ```

  这一部分设置了history server要去哪里找spark的日志（和上面的日志输出路径保持一致即可）
  
  `除了配置在spark-env.sh，还可以将参数直接配置在spark-defaults.conf`

配置结束了，现在去spark/sbin目录下，运行 start-history-server.sh 即可拉起web ui服务。

[参考资料](https://blog.csdn.net/XIAOMO__/article/details/107988164){:target="_blank"}



## 源码特性

### 1、whole stage code gen

Spark中的Whole-Stage Code Generation（整阶段代码生成）是一种优化技术，用于提高Spark SQL查询的性能。它的主要思想是将一整个查询阶段的代码生成为一个单独的函数，以减少函数调用和数据序列化/反序列化的开销。

在Spark中，查询通常由多个操作组成，例如选择、过滤、聚合等。每个操作都可以表示为一个函数，但这些函数通常会导致较多的函数调用开销和数据的序列化/反序列化开销。为了优化这一点，Spark使用Whole-Stage Code Generation将多个操作合并为一个函数，并生成高效的Java字节码来执行这个函数。

整阶段代码生成的过程大致如下：

1. Spark SQL将逻辑查询计划转换为物理查询计划。
2. 物理查询计划被划分为一系列称为“物理子计划”的阶段，每个子计划包含一组可以一起执行的操作。
3. 对于每个物理子计划，Spark生成一个包含所有操作的单个函数，并为该函数生成优化的Java字节码。
4. 在运行时，生成的函数被传递给Spark执行引擎执行，从而减少了函数调用和数据的序列化/反序列化开销。

整阶段代码生成可以大大提高Spark SQL查询的性能，特别是在涉及大规模数据处理时。通过减少函数调用和数据的序列化/反序列化开销，整阶段代码生成可以显著提升查询的执行速度。

### 2、cloumnar

在Spark中，"columnar" 通常用于描述一种数据存储和处理的方式，即按列存储。与传统的按行存储相比，按列存储将同一列的数据连续存储在一起，而不是将整行数据连续存储。这种存储方式在数据分析和处理中具有一些优势，特别是对于大规模数据集。

在 Spark SQL 中，列式存储通常指的是 Parquet 格式。Parquet 是一种列式存储格式，它将数据按列存储在文件中，这样做有几个好处：

1. **高压缩率**：相同的值在列中是连续的，这样可以更容易地实现高效的压缩，减少存储空间的使用。
2. **只读查询效率高**：当查询只需要访问部分列时，只需要读取这些列的数据，而不必读取整行数据，这可以减少读取的数据量，提高查询效率。
3. **谓词下推**：列式存储使得更容易进行谓词下推，即在数据存储时就可以排除不满足条件的数据，减少查询时需要处理的数据量。

Spark SQL 在处理 Parquet 格式的数据时，会利用列式存储的优势来提高查询性能和降低资源消耗。例如，当使用整阶段代码生成来执行查询时，Spark 可以有效地处理列式存储的数据，从而加速查询的执行。

总的来说，列式存储在 Spark 中被广泛应用于大规模数据处理场景，可以提供更高效的数据存储和处理方式，特别是对于分析型工作负载。

### 3、AQE

通过配置`spark.sql.adaptive.enabled=true;`开启AQE

在Apache Spark中，AQE（Adaptive Query Execution，自适应查询执行）是一种优化技术，旨在根据作业的运行情况动态地调整执行计划，以提高性能并降低资源消耗。AQE可以在运行时根据数据的分布、任务的执行情况和资源可用性等因素进行优化，从而更好地适应不同的工作负载和环境。

AQE的主要特点和功能包括：

1. **动态优化执行计划**：根据作业的运行情况，AQE可以动态地调整执行计划，例如更改连接策略、重新分区或更改过滤器等，以提高查询性能。
2. **自适应分区**：根据数据分布和操作的需要，AQE可以自动调整数据的分区方式，以减少数据倾斜并提高并行性。
3. **动态过滤器推断**：AQE可以根据运行时数据的统计信息推断出更好的过滤器，以减少需要处理的数据量。
4. **自适应扫描**：根据数据分布和过滤器条件，AQE可以选择合适的扫描方式，例如跳过不需要的数据块或者选择更有效的扫描方式。
5. **动态重分区**：根据数据倾斜和任务执行情况，AQE可以动态地重新分区数据，以平衡任务负载并提高并行性能。

通过这些功能，AQE可以在不同的查询场景下自动调整执行计划，从而提高查询性能和资源利用率。AQE是Spark 3.0中引入的功能，并在后续版本中不断改进和优化，成为了Spark SQL中重要的优化技术之一。

### 4、Origin

Orgin用于存储行号、sql文本、起止位置等

CurrentOrigin中有一个value变量，本质是一个LocalThread[Origin]

常看到下面的代码：

```scala
CurrentOrigin.withOrigin(origin) {
	 要执行的操作
}
// withOrigin做的事情是，如果“要执行的操作”正常执行（没有报错，没有try到异常），则设置新的orgin，否则恢复原来的origin。
```

`CurrentOrigin.withOrigin` 是 Spark 中的一个方法，用于在当前线程上设置一个新的 SQL 分析上下文（`SQLContext`）的 origin。在 Spark 中，origin 用于跟踪和诊断 SQL 查询或操作的来源，以便在日志或错误消息中提供更多的上下文信息。

该方法的签名通常是这样的：

```scala
def withOrigin[T](origin: Origin)(f: => T): T
```

其中，`origin` 是一个表示查询或操作来源的对象，通常包含文件名、行号等信息。`f` 是一个函数，表示需要在设置了新的 origin 的上下文中执行的代码块。该方法会返回 `f` 的结果。

使用 `CurrentOrigin.withOrigin` 方法可以在 Spark 中为特定的操作设置 origin，以便在日志中能够更容易地追踪和诊断问题。

### 5、ORC格式（列式存储）

ORC（Optimized Row Columnar）是一种优化的列式存储文件格式，通常用于大数据处理系统中，例如Apache Hive和Apache Spark等。ORC格式旨在提供高效的数据压缩和快速的读取性能，特别适用于大规模数据集的存储和分析。

ORC格式的主要特点和优势包括：

1. **列式存储**：将数据按列存储，相同类型的数据被连续地存储在一起，这样可以实现更好的压缩率和查询性能。
2. **分区存储**：数据可以按照用户定义的分区方式进行存储，这样可以提高查询效率，特别是在分析时只需要部分数据的情况下。
3. **压缩**：ORC格式支持多种压缩算法，例如Zlib、Snappy、LZO等，可以根据需要选择合适的压缩算法来减少存储空间。
4. **列统计信息**：ORC文件中包含了每列的统计信息，包括最小值、最大值、空值数量等，这些信息可以帮助查询优化器更好地执行查询计划。
5. **谓词下推**：ORC格式支持谓词下推（Predicate Pushdown），可以在读取数据时将过滤条件应用于存储文件，减少需要读取和处理的数据量。
6. **数据跳过**：ORC格式支持在读取数据时跳过不必要的行或列，从而进一步提高读取性能。

总的来说，ORC格式在大数据处理系统中被广泛应用，可以提供高效的数据存储和查询性能，特别适用于需要处理大规模数据集的场景。

### 6、Spark Shuffle

Spark 中的 Shuffle 是一个关键过程，用于跨节点重新分配数据，以确保下一阶段的任务可以获得所需的数据。Shuffle 过程通常在诸如 `groupByKey`、`reduceByKey`、`join`、`cogroup` 等操作中触发。Shuffle 过程包括以下几个主要步骤：

#### Shuffle 过程详解

1. **Map 阶段 (Shuffle Write)**
2. **Shuffle 阶段**
3. **Reduce 阶段 (Shuffle Read)**

##### 1. Map 阶段 (Shuffle Write)

在 Map 阶段，每个任务会将输入数据进行处理，并将结果数据根据目标分区进行拆分和存储。

- **步骤**：
  1. **Map Task 执行**：每个 Executor 上的 Map 任务对输入数据进行处理，生成键值对 (key-value pairs)。
  2. **分区 (Partitioning)**：将输出数据按照分区函数 (如 HashPartitioner 或 RangePartitioner) 分成多个分区。
  3. **溢写 (Spill)**：如果内存不足，数据会被溢写到磁盘。
  4. **排序 (Sorting)**：将每个分区内的数据按照键进行排序。
  5. **数据写入本地磁盘**：最终，排序后的数据会被写入到本地磁盘的临时文件中。每个分区对应一个文件，这些文件将被下游的 Reduce 任务读取。
- **优化**：使用 `spark.shuffle.file.buffer` 和 `spark.shuffle.sort.bypassMergeThreshold` 等参数可以优化 Shuffle Write 的性能。

##### 2. Shuffle 阶段

Shuffle 阶段是 Map 阶段和 Reduce 阶段之间的数据传输过程。此阶段的主要任务是将每个 Map 任务的输出数据传输给需要这些数据的 Reduce 任务。

- **步骤**：
  1. **数据索引**：每个 Map 任务的输出数据被分区和排序后，生成一个索引文件，记录每个分区的数据位置。
  2. **数据传输**：Reduce 任务根据需要，通过网络从各个 Map 任务所在的 Executor 拉取相应的分区数据。
- **优化**：使用 `spark.reducer.maxSizeInFlight` 和 `spark.shuffle.io.retryWait` 等参数可以优化 Shuffle 数据传输的性能和可靠性。

##### 3. Reduce 阶段 (Shuffle Read)

在 Reduce 阶段，每个任务从多个 Map 任务所在的 Executor 拉取数据，对数据进行聚合或其他操作。

- **步骤**：
  1. **数据拉取 (Fetch)**：每个 Reduce 任务根据需要，从所有 Map 任务所在的 Executor 上拉取相应的分区数据。
  2. **合并 (Merge)**：拉取的数据在 Reduce 任务所在的 Executor 本地进行合并。合并过程中会对数据进行排序和聚合。
  3. **Reduce Task 执行**：Reduce 任务对合并后的数据进行最终处理和计算，生成输出结果。
- **优化**：使用 `spark.shuffle.compress` 和 `spark.shuffle.spill.compress` 等参数可以优化 Shuffle Read 的性能。

#### Shuffle 过程示意图

假设我们有一个简单的 `reduceByKey` 操作，整个 Shuffle 过程可以示意如下：

1. **Map 阶段**：
   - 输入数据：[(1, 2), (3, 4), (3, 6), (1, 4)]
   - Map 任务输出并分区：[(1, 2), (1, 4)] 到分区 0，[(3, 4), (3, 6)] 到分区 1
2. **Shuffle 阶段**：
   - 分区 0 的数据从多个 Map 任务中传输到目标 Reduce 任务所在的 Executor。
   - 分区 1 的数据从多个 Map 任务中传输到目标 Reduce 任务所在的 Executor。
3. **Reduce 阶段**：
   - Reduce 任务拉取并合并分区 0 的数据 [(1, 2), (1, 4)]，并进行聚合得到 (1, 6)。
   - Reduce 任务拉取并合并分区 1 的数据 [(3, 4), (3, 6)]，并进行聚合得到 (3, 10)。

#### 优化 Shuffle 的建议

1. **数据倾斜 (Data Skew)**：
   - **问题**：数据分布不均衡，导致某些分区的数据量远大于其他分区，造成任务瓶颈。
   - **解决方案**：使用自定义分区器、增加随机前缀键或使用 Spark 提供的 `salting` 技术进行数据分散。
2. **网络传输优化**：
   - **问题**：Shuffle 过程中的网络传输开销较大。
   - **解决方案**：调整 `spark.reducer.maxSizeInFlight` 和 `spark.shuffle.compress` 等参数，提高网络传输效率。
3. **内存管理**：
   - **问题**：Shuffle 数据量大时可能会导致内存溢出。
   - **解决方案**：适当调大 `spark.memory.fraction` 和 `spark.shuffle.spill` 参数，并使用外部 shuffle 服务。
4. **持久化和缓存**：
   - **问题**：频繁的 Shuffle 操作会影响性能。
   - **解决方案**：适当使用 `persist` 或 `cache` 操作，将中间结果缓存到内存或磁盘，减少重复计算。

#### 总结

Shuffle 过程在 Spark 中非常关键，涉及到数据的分区、排序、传输和聚合。通过了解和优化 Shuffle 过程，可以显著提高 Spark 作业的性能和稳定性。





## 源码解读

### 1、FileSourceScanExec

*`partition columns` 在 `FileSourceScanExec` 中的作用*

1. **分区裁剪（Partition Pruning）**：通过分区列，Spark 可以在查询时只读取相关的分区，而不是扫描所有文件。例如，如果数据按日期分区，并且查询只涉及特定日期范围内的数据，Spark 只会读取这些日期对应的分区，从而减少了数据读取量和I/O操作。
2. **优化查询性能**：利用分区列进行查询时，Spark 可以更高效地执行过滤操作，从而提高查询的整体性能。
3. **减少数据倾斜**：通过合理的分区设计，可以避免数据倾斜的问题，提高查询的并行度和执行效率。

*如何使用 `partition columns`*

在定义数据源表时，可以指定分区列。例如，在使用 Spark SQL 时，可以通过 `PARTITIONED BY` 关键字来指定分区列：

```sql
CREATE TABLE sales (
  product STRING,
  revenue FLOAT,
  date STRING
)
USING parquet
PARTITIONED BY (date);
```

在上面的示例中，`date` 列被用作分区列。当查询涉及 `date` 列时，Spark 可以利用分区信息进行分区裁剪。

*`FileSourceScanExec` 中的分区列示例*

假设我们有一个 Parquet 文件目录结构如下：

```verilog
/data/sales/date=2023-01-01/part-00000.parquet
/data/sales/date=2023-01-02/part-00000.parquet
/data/sales/date=2023-01-03/part-00000.parquet
```

当我们查询特定日期的数据时：

```sql
SELECT * FROM sales WHERE date = '2023-01-02';
```

Spark 会生成一个物理执行计划，其中包含 `FileSourceScanExec` 节点，并且只会扫描 `date=2023-01-02` 对应的文件。`partition columns` 在这个节点中用于指示哪些列是分区列，并用于分区裁剪。

*查看执行计划中的 `partition columns`*

要查看执行计划，可以使用 `explain` 方法：

```scala
val df = spark.read.parquet("/data/sales")
df.filter("date = '2023-01-02'").explain(true)
```

执行上述代码时，会输出详细的物理计划，其中包含 `FileSourceScanExec` 节点及其属性，包括 `partition columns`。

*`pushedDownFilters` 的作用*

1. **减少数据读取量**：通过将过滤器下推到数据源，Spark 可以在数据读取阶段就过滤掉不必要的数据，减少了需要传输和处理的数据量。
2. **提高查询性能**：在数据源层面应用过滤器，可以减少 Spark 需要处理的数据量，从而加快查询执行速度。
3. **优化 I/O 操作**：通过在数据源层面进行过滤，可以减少不必要的 I/O 操作，提高整个数据处理管道的效率。

*示例*

假设我们有一个包含销售数据的 Parquet 文件，其中包含以下列：

- `product`
- `revenue`
- `date`

现在我们要查询特定日期的数据：

```scala
val df = spark.read.parquet("/data/sales")
val filteredDf = df.filter("date = '2023-01-02'")
filteredDf.explain(true)
```

当我们调用 `explain(true)` 方法查看详细的物理执行计划时，输出中会包含 `FileSourceScanExec` 节点，其中 `pushedDownFilters` 属性会显示哪些过滤器被下推到了数据源。

示例输出（简化版）可能如下：

```verilog
== Physical Plan ==
*(1) FileScan parquet [product#0,revenue#1,date#2] Batched: true, DataFilters: [isnotnull(date#2), (date#2 = 2023-01-02)], Format: Parquet, Location: InMemoryFileIndex[file:/data/sales], PartitionFilters: [], PushedFilters: [IsNotNull(date), EqualTo(date,2023-01-02)], ReadSchema: struct<product:string,revenue:double,date:string>
```

在这个输出中：

- `PushedFilters: [IsNotNull(date), EqualTo(date,2023-01-02)]` 表示 `pushedDownFilters`。这些过滤器已经下推到了数据源，在数据读取时应用。
- `DataFilters: [isnotnull(date#2), (date#2 = 2023-01-02)]` 表示在 Spark 执行计划中应用的过滤器。







## [配置项](https://spark.apache.org/docs/latest/configuration.html){:target="_blank"}

### Broadcast阈值

spark.sql.autoBroadcastJoinThreshold

设置为-1，则禁用Broadcast Hash Join
