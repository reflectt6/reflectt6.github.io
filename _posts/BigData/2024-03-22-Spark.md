---
layout: page-with-sidebar
title:  "Spark"
date:   2024-03-22 14:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
hideTag: false
---

## 安装部署

Spark有几种集群部署模式，具体请看[Cluster Mode Overview](https://spark.apache.org/docs/latest/cluster-overview.html){:target="_blank"}

比较关键的组件有Driver和Worker，见下图

![image-20240422143309188](/assets/images/2024-03-22-Spark//image-20240422143309188.png)

集中部署模式的区别在于Cluster Manager的不同

### Cluster Manager Types

The system currently supports several cluster managers:
系统目前支持多种集群管理器：

- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html){:target="_blank"} – a simple cluster manager included with Spark that makes it easy to set up a cluster.
  Standalone——Spark 附带的一个简单的集群管理器，可以轻松设置集群。

  要使用启动脚本启动 Spark 独立集群，您应该在 Spark 目录中创建一个名为 conf/workers 的文件，该文件必须包含您打算启动 Spark Worker 的所有计算机的主机名，每行一个。如果conf/workers不存在，启动脚本默认为单台机器（localhost），这对于测试很有用。请注意，主机通过 ssh 访问每台工作机。默认情况下，ssh 并行运行，并且需要设置无密码（使用私钥）访问。如果您没有无密码设置，则可以设置环境变量 SPARK_SSH_FOREGROUND 并连续为每个工作人员提供密码。

- [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html){:target="_blank"} – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)
  Apache Mesos – 通用集群管理器，还可以运行 Hadoop MapReduce 和服务应用程序。 （已弃用）

- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html){:target="_blank"} – the resource manager in Hadoop 3.
  Hadoop YARN – Hadoop 3 中的资源管理器。

- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html){:target="_blank"} – an open-source system for automating deployment, scaling, and management of containerized applications.
  Kubernetes – 一个用于自动化部署、扩展和管理容器化应用程序的开源系统。

### [Submitting Applications](https://spark.apache.org/docs/latest/submitting-applications.html){:target="_blank"}

Spark通过shell脚本提交任务，使用脚本参数可以随意设置任务运行在哪一种部署模式下，常见参数如下：

- `--class`: The entry point for your application (e.g. `org.apache.spark.examples.SparkPi`)
  `--class` ：应用程序的入口点（例如 `org.apache.spark.examples.SparkPi` ）

- `--master`: The [master URL](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls){:target="_blank"} for the cluster (e.g. `spark://23.195.26.187:7077`)

  Master URLs有哪些呢？如下表

  The master URL passed to Spark can be in one of the following formats:

  | Master URL                        | Meaning                                                      |
  | --------------------------------- | ------------------------------------------------------------ |
  | `local`                           | Run Spark locally with one worker thread (i.e. no parallelism at all). |
  | `local[K]`                        | Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). |
  | `local[K,F]`                      | Run Spark locally with K worker threads and F maxFailures (see [spark.task.maxFailures](https://spark.apache.org/docs/latest/configuration.html#scheduling){:target="_blank"} for an explanation of this variable). |
  | `local[*]`                        | Run Spark locally with as many worker threads as logical cores on your machine. |
  | `local[*,F]`                      | Run Spark locally with as many worker threads as logical cores on your machine and F maxFailures. |
  | `local-cluster[N,C,M]`            | Local-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker. |
  | `spark://HOST:PORT`               | Connect to the given [Spark standalone cluster](https://spark.apache.org/docs/latest/spark-standalone.html){:target="_blank"} master. The port must be whichever one your master is configured to use, which is 7077 by default. |
  | `spark://HOST1:PORT1,HOST2:PORT2` | Connect to the given [Spark standalone cluster with standby masters with Zookeeper](https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper){:target="_blank"}. The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default. |
  | `mesos://HOST:PORT`               | Connect to the given [Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html){:target="_blank"} cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use `mesos://zk://...`. To submit with `--deploy-mode cluster`, the HOST:PORT should be configured to connect to the [MesosClusterDispatcher](https://spark.apache.org/docs/latest/running-on-mesos.html#cluster-mode){:target="_blank"}. |
  | `yarn`                            | Connect to a [YARN ](https://spark.apache.org/docs/latest/running-on-yarn.html){:target="_blank"}cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The cluster location will be found based on the `HADOOP_CONF_DIR` or `YARN_CONF_DIR` variable. |
  | `k8s://HOST:PORT`                 | Connect to a [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html){:target="_blank"} cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The `HOST` and `PORT` refer to the [Kubernetes API Server](https://kubernetes.io/docs/reference/generated/kube-apiserver/){:target="_blank"}. It connects using TLS by default. In order to force it to use an unsecured connection, you can use `k8s://http://HOST:PORT`. |

- `--deploy-mode`: Whether to deploy your driver on the worker nodes (`cluster`) or locally as an external client (`client`) (default: `client`) **†**
  `--deploy-mode` ：是将驱动程序部署在工作节点 ( `cluster` ) 上还是作为外部客户端部署在本地 ( `client` )（默认值： `client` ) †

- `--conf`: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. `--conf <key>=<value> --conf <key2>=<value2>`)
  `--conf` ：任意 Spark 配置属性，采用 key=value 格式。对于包含空格的值，请将“key=value”括在引号中（如图所示）。多个配置应作为单独的参数传递。 （例如 `--conf <key>=<value> --conf <key2>=<value2>` ）

- `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.
  `application-jar` ：捆绑 jar 的路径，包括您的应用程序和所有依赖项。 URL 必须在集群内部全局可见，例如，所有节点上都存在的 `hdfs://` 路径或 `file://` 路径。

- `application-arguments`: Arguments passed to the main method of your main class, if any
  `application-arguments` ：传递给主类的 main 方法的参数（如果有）
  
- --conf spark.driver.extraClassPath=/opt/
  
  driver端额外需要添加的Java包
  
- --conf spark.executor.extraClassPath=/opt/

  executor端额外需要添加的Java包

- --conf spark.driverEnv.LD_LIBRARY_PATH=/opt/

  driver端额外需要添加的C++库

- --conf spark.executorEnv.LD_LIBRARY_PATH=/opt/

  Executor端额外需要添加的C++库

- --conf spark.driverEnv.OMNI_HOME=/opt/

  driver端额外需要添加的C++库路径 “OMNI_HOME”是自定义的环境变量名，用于c++代码加载自定义类库

- --conf spark.executorEnv.OMNI_HOME=/opt/

  Executor端额外需要添加的C++库路径 “OMNI_HOME”是自定义的环境变量名，用于c++代码加载自定义类库


## 常见部署问题

### 1、Spark Web UI访问不了

Spark Web UI可以看到历史执行的sql以及对应的执行计划、metrics等，对于开发很重要。假设Web UI的端口为18080，有时候我们无法访问他：

1、检查代理，在公司内网上外网需要代理，但是往往Spark部署在内网集群上，内网机器是不需要代理的。这时我们在windows上输入“代理”，打开代理服务器设置。在`请勿对以下列条目开头的地址使用代理服务器。。。`后面加上内网机器的IP。

2、检查集群里的HistoryServer有没有启动：

HistoryServer这个进程就是Web UI的服务进程，如果没有启动，是肯定访问不了web ui的。

配置history server：主要分两部分

- 第一部分：配置spark-defaults.conf

  ```conf
  spark.eventLog.enabled   true
  spark.eventLog.dir       hdfs://node7-2:8020/directory
  ```

  这个设置表示打开spark的日志系统，并设置日志输出目录。（没有日志输出，web ui也是用不了，web ui本质上是把磁盘的log文件可视化）

- 第二部分：设置spark-env.sh

  ```bash
  
   export SPARK_HISTORY_OPTS="
  -Dspark.history.ui.port=18080
  -Dspark.history.retainedApplications=30
  -Dspark.history.fs.logDirectory=hdfs://node7-2:8020/directory"
  ```

  这一部分设置了history server要去哪里找spark的日志（和上面的日志输出路径保持一致即可）
  
  `除了配置在spark-env.sh，还可以将参数直接配置在spark-defaults.conf`

配置结束了，现在去spark/sbin目录下，运行 start-history-server.sh 即可拉起web ui服务。

[参考资料](https://blog.csdn.net/XIAOMO__/article/details/107988164){:target="_blank"}



## 源码特性

### 1、whole stage code gen

Spark中的Whole-Stage Code Generation（整阶段代码生成）是一种优化技术，用于提高Spark SQL查询的性能。它的主要思想是将一整个查询阶段的代码生成为一个单独的函数，以减少函数调用和数据序列化/反序列化的开销。

在Spark中，查询通常由多个操作组成，例如选择、过滤、聚合等。每个操作都可以表示为一个函数，但这些函数通常会导致较多的函数调用开销和数据的序列化/反序列化开销。为了优化这一点，Spark使用Whole-Stage Code Generation将多个操作合并为一个函数，并生成高效的Java字节码来执行这个函数。

整阶段代码生成的过程大致如下：

1. Spark SQL将逻辑查询计划转换为物理查询计划。
2. 物理查询计划被划分为一系列称为“物理子计划”的阶段，每个子计划包含一组可以一起执行的操作。
3. 对于每个物理子计划，Spark生成一个包含所有操作的单个函数，并为该函数生成优化的Java字节码。
4. 在运行时，生成的函数被传递给Spark执行引擎执行，从而减少了函数调用和数据的序列化/反序列化开销。

整阶段代码生成可以大大提高Spark SQL查询的性能，特别是在涉及大规模数据处理时。通过减少函数调用和数据的序列化/反序列化开销，整阶段代码生成可以显著提升查询的执行速度。

### 2、cloumnar

在Spark中，"columnar" 通常用于描述一种数据存储和处理的方式，即按列存储。与传统的按行存储相比，按列存储将同一列的数据连续存储在一起，而不是将整行数据连续存储。这种存储方式在数据分析和处理中具有一些优势，特别是对于大规模数据集。

在 Spark SQL 中，列式存储通常指的是 Parquet 格式。Parquet 是一种列式存储格式，它将数据按列存储在文件中，这样做有几个好处：

1. **高压缩率**：相同的值在列中是连续的，这样可以更容易地实现高效的压缩，减少存储空间的使用。
2. **只读查询效率高**：当查询只需要访问部分列时，只需要读取这些列的数据，而不必读取整行数据，这可以减少读取的数据量，提高查询效率。
3. **谓词下推**：列式存储使得更容易进行谓词下推，即在数据存储时就可以排除不满足条件的数据，减少查询时需要处理的数据量。

Spark SQL 在处理 Parquet 格式的数据时，会利用列式存储的优势来提高查询性能和降低资源消耗。例如，当使用整阶段代码生成来执行查询时，Spark 可以有效地处理列式存储的数据，从而加速查询的执行。

总的来说，列式存储在 Spark 中被广泛应用于大规模数据处理场景，可以提供更高效的数据存储和处理方式，特别是对于分析型工作负载。

### 3、AQE

通过配置`spark.sql.adaptive.enabled=true;`开启AQE

在Apache Spark中，AQE（Adaptive Query Execution，自适应查询执行）是一种优化技术，旨在根据作业的运行情况动态地调整执行计划，以提高性能并降低资源消耗。AQE可以在运行时根据数据的分布、任务的执行情况和资源可用性等因素进行优化，从而更好地适应不同的工作负载和环境。

AQE的主要特点和功能包括：

1. **动态优化执行计划**：根据作业的运行情况，AQE可以动态地调整执行计划，例如更改连接策略、重新分区或更改过滤器等，以提高查询性能。
2. **自适应分区**：根据数据分布和操作的需要，AQE可以自动调整数据的分区方式，以减少数据倾斜并提高并行性。
3. **动态过滤器推断**：AQE可以根据运行时数据的统计信息推断出更好的过滤器，以减少需要处理的数据量。
4. **自适应扫描**：根据数据分布和过滤器条件，AQE可以选择合适的扫描方式，例如跳过不需要的数据块或者选择更有效的扫描方式。
5. **动态重分区**：根据数据倾斜和任务执行情况，AQE可以动态地重新分区数据，以平衡任务负载并提高并行性能。

通过这些功能，AQE可以在不同的查询场景下自动调整执行计划，从而提高查询性能和资源利用率。AQE是Spark 3.0中引入的功能，并在后续版本中不断改进和优化，成为了Spark SQL中重要的优化技术之一。

### 4、Origin

Orgin用于存储行号、sql文本、起止位置等

CurrentOrigin中有一个value变量，本质是一个LocalThread[Origin]

常看到下面的代码：

```scala
CurrentOrigin.withOrigin(origin) {
	 要执行的操作
}
// withOrigin做的事情是，如果“要执行的操作”正常执行（没有报错，没有try到异常），则设置新的orgin，否则恢复原来的origin。
```

`CurrentOrigin.withOrigin` 是 Spark 中的一个方法，用于在当前线程上设置一个新的 SQL 分析上下文（`SQLContext`）的 origin。在 Spark 中，origin 用于跟踪和诊断 SQL 查询或操作的来源，以便在日志或错误消息中提供更多的上下文信息。

该方法的签名通常是这样的：

```scala
def withOrigin[T](origin: Origin)(f: => T): T
```

其中，`origin` 是一个表示查询或操作来源的对象，通常包含文件名、行号等信息。`f` 是一个函数，表示需要在设置了新的 origin 的上下文中执行的代码块。该方法会返回 `f` 的结果。

使用 `CurrentOrigin.withOrigin` 方法可以在 Spark 中为特定的操作设置 origin，以便在日志中能够更容易地追踪和诊断问题。

### 5、ORC格式（列式存储）

ORC（Optimized Row Columnar）是一种优化的列式存储文件格式，通常用于大数据处理系统中，例如Apache Hive和Apache Spark等。ORC格式旨在提供高效的数据压缩和快速的读取性能，特别适用于大规模数据集的存储和分析。

ORC格式的主要特点和优势包括：

1. **列式存储**：将数据按列存储，相同类型的数据被连续地存储在一起，这样可以实现更好的压缩率和查询性能。
2. **分区存储**：数据可以按照用户定义的分区方式进行存储，这样可以提高查询效率，特别是在分析时只需要部分数据的情况下。
3. **压缩**：ORC格式支持多种压缩算法，例如Zlib、Snappy、LZO等，可以根据需要选择合适的压缩算法来减少存储空间。
4. **列统计信息**：ORC文件中包含了每列的统计信息，包括最小值、最大值、空值数量等，这些信息可以帮助查询优化器更好地执行查询计划。
5. **谓词下推**：ORC格式支持谓词下推（Predicate Pushdown），可以在读取数据时将过滤条件应用于存储文件，减少需要读取和处理的数据量。
6. **数据跳过**：ORC格式支持在读取数据时跳过不必要的行或列，从而进一步提高读取性能。

总的来说，ORC格式在大数据处理系统中被广泛应用，可以提供高效的数据存储和查询性能，特别适用于需要处理大规模数据集的场景。

### 6、Spark Shuffle

Spark 中的 Shuffle 是一个关键过程，用于跨节点重新分配数据，以确保下一阶段的任务可以获得所需的数据。Shuffle 过程通常在诸如 `groupByKey`、`reduceByKey`、`join`、`cogroup` 等操作中触发。Shuffle 过程包括以下几个主要步骤：

#### Shuffle 过程详解

1. **Map 阶段 (Shuffle Write)**
2. **Shuffle 阶段**
3. **Reduce 阶段 (Shuffle Read)**

##### 1. Map 阶段 (Shuffle Write)

在 Map 阶段，每个任务会将输入数据进行处理，并将结果数据根据目标分区进行拆分和存储。

- **步骤**：
  1. **Map Task 执行**：每个 Executor 上的 Map 任务对输入数据进行处理，生成键值对 (key-value pairs)。
  2. **分区 (Partitioning)**：将输出数据按照分区函数 (如 HashPartitioner 或 RangePartitioner) 分成多个分区。
  3. **溢写 (Spill)**：如果内存不足，数据会被溢写到磁盘。
  4. **排序 (Sorting)**：将每个分区内的数据按照键进行排序。
  5. **数据写入本地磁盘**：最终，排序后的数据会被写入到本地磁盘的临时文件中。每个分区对应一个文件，这些文件将被下游的 Reduce 任务读取。
- **优化**：使用 `spark.shuffle.file.buffer` 和 `spark.shuffle.sort.bypassMergeThreshold` 等参数可以优化 Shuffle Write 的性能。

##### 2. Shuffle 阶段

Shuffle 阶段是 Map 阶段和 Reduce 阶段之间的数据传输过程。此阶段的主要任务是将每个 Map 任务的输出数据传输给需要这些数据的 Reduce 任务。

- **步骤**：
  1. **数据索引**：每个 Map 任务的输出数据被分区和排序后，生成一个索引文件，记录每个分区的数据位置。
  2. **数据传输**：Reduce 任务根据需要，通过网络从各个 Map 任务所在的 Executor 拉取相应的分区数据。
- **优化**：使用 `spark.reducer.maxSizeInFlight` 和 `spark.shuffle.io.retryWait` 等参数可以优化 Shuffle 数据传输的性能和可靠性。

##### 3. Reduce 阶段 (Shuffle Read)

在 Reduce 阶段，每个任务从多个 Map 任务所在的 Executor 拉取数据，对数据进行聚合或其他操作。

- **步骤**：
  1. **数据拉取 (Fetch)**：每个 Reduce 任务根据需要，从所有 Map 任务所在的 Executor 上拉取相应的分区数据。
  2. **合并 (Merge)**：拉取的数据在 Reduce 任务所在的 Executor 本地进行合并。合并过程中会对数据进行排序和聚合。
  3. **Reduce Task 执行**：Reduce 任务对合并后的数据进行最终处理和计算，生成输出结果。
- **优化**：使用 `spark.shuffle.compress` 和 `spark.shuffle.spill.compress` 等参数可以优化 Shuffle Read 的性能。

#### Shuffle 过程示意图

假设我们有一个简单的 `reduceByKey` 操作，整个 Shuffle 过程可以示意如下：

1. **Map 阶段**：
   - 输入数据：[(1, 2), (3, 4), (3, 6), (1, 4)]
   - Map 任务输出并分区：[(1, 2), (1, 4)] 到分区 0，[(3, 4), (3, 6)] 到分区 1
2. **Shuffle 阶段**：
   - 分区 0 的数据从多个 Map 任务中传输到目标 Reduce 任务所在的 Executor。
   - 分区 1 的数据从多个 Map 任务中传输到目标 Reduce 任务所在的 Executor。
3. **Reduce 阶段**：
   - Reduce 任务拉取并合并分区 0 的数据 [(1, 2), (1, 4)]，并进行聚合得到 (1, 6)。
   - Reduce 任务拉取并合并分区 1 的数据 [(3, 4), (3, 6)]，并进行聚合得到 (3, 10)。

#### 优化 Shuffle 的建议

1. **数据倾斜 (Data Skew)**：
   - **问题**：数据分布不均衡，导致某些分区的数据量远大于其他分区，造成任务瓶颈。
   - **解决方案**：使用自定义分区器、增加随机前缀键或使用 Spark 提供的 `salting` 技术进行数据分散。
2. **网络传输优化**：
   - **问题**：Shuffle 过程中的网络传输开销较大。
   - **解决方案**：调整 `spark.reducer.maxSizeInFlight` 和 `spark.shuffle.compress` 等参数，提高网络传输效率。
3. **内存管理**：
   - **问题**：Shuffle 数据量大时可能会导致内存溢出。
   - **解决方案**：适当调大 `spark.memory.fraction` 和 `spark.shuffle.spill` 参数，并使用外部 shuffle 服务。
4. **持久化和缓存**：
   - **问题**：频繁的 Shuffle 操作会影响性能。
   - **解决方案**：适当使用 `persist` 或 `cache` 操作，将中间结果缓存到内存或磁盘，减少重复计算。

#### 总结

Shuffle 过程在 Spark 中非常关键，涉及到数据的分区、排序、传输和聚合。通过了解和优化 Shuffle 过程，可以显著提高 Spark 作业的性能和稳定性。





## 源码解读

### 1、FileSourceScanExec

*`partition columns` 在 `FileSourceScanExec` 中的作用*

1. **分区裁剪（Partition Pruning）**：通过分区列，Spark 可以在查询时只读取相关的分区，而不是扫描所有文件。例如，如果数据按日期分区，并且查询只涉及特定日期范围内的数据，Spark 只会读取这些日期对应的分区，从而减少了数据读取量和I/O操作。
2. **优化查询性能**：利用分区列进行查询时，Spark 可以更高效地执行过滤操作，从而提高查询的整体性能。
3. **减少数据倾斜**：通过合理的分区设计，可以避免数据倾斜的问题，提高查询的并行度和执行效率。

*如何使用 `partition columns`*

在定义数据源表时，可以指定分区列。例如，在使用 Spark SQL 时，可以通过 `PARTITIONED BY` 关键字来指定分区列：

```sql
CREATE TABLE sales (
  product STRING,
  revenue FLOAT,
  date STRING
)
USING parquet
PARTITIONED BY (date);
```

在上面的示例中，`date` 列被用作分区列。当查询涉及 `date` 列时，Spark 可以利用分区信息进行分区裁剪。

*`FileSourceScanExec` 中的分区列示例*

假设我们有一个 Parquet 文件目录结构如下：

```verilog
/data/sales/date=2023-01-01/part-00000.parquet
/data/sales/date=2023-01-02/part-00000.parquet
/data/sales/date=2023-01-03/part-00000.parquet
```

当我们查询特定日期的数据时：

```sql
SELECT * FROM sales WHERE date = '2023-01-02';
```

Spark 会生成一个物理执行计划，其中包含 `FileSourceScanExec` 节点，并且只会扫描 `date=2023-01-02` 对应的文件。`partition columns` 在这个节点中用于指示哪些列是分区列，并用于分区裁剪。

*查看执行计划中的 `partition columns`*

要查看执行计划，可以使用 `explain` 方法：

```scala
val df = spark.read.parquet("/data/sales")
df.filter("date = '2023-01-02'").explain(true)
```

执行上述代码时，会输出详细的物理计划，其中包含 `FileSourceScanExec` 节点及其属性，包括 `partition columns`。

*`pushedDownFilters` 的作用*

1. **减少数据读取量**：通过将过滤器下推到数据源，Spark 可以在数据读取阶段就过滤掉不必要的数据，减少了需要传输和处理的数据量。
2. **提高查询性能**：在数据源层面应用过滤器，可以减少 Spark 需要处理的数据量，从而加快查询执行速度。
3. **优化 I/O 操作**：通过在数据源层面进行过滤，可以减少不必要的 I/O 操作，提高整个数据处理管道的效率。

*示例*

假设我们有一个包含销售数据的 Parquet 文件，其中包含以下列：

- `product`
- `revenue`
- `date`

现在我们要查询特定日期的数据：

```scala
val df = spark.read.parquet("/data/sales")
val filteredDf = df.filter("date = '2023-01-02'")
filteredDf.explain(true)
```

当我们调用 `explain(true)` 方法查看详细的物理执行计划时，输出中会包含 `FileSourceScanExec` 节点，其中 `pushedDownFilters` 属性会显示哪些过滤器被下推到了数据源。

示例输出（简化版）可能如下：

```verilog
== Physical Plan ==
*(1) FileScan parquet [product#0,revenue#1,date#2] Batched: true, DataFilters: [isnotnull(date#2), (date#2 = 2023-01-02)], Format: Parquet, Location: InMemoryFileIndex[file:/data/sales], PartitionFilters: [], PushedFilters: [IsNotNull(date), EqualTo(date,2023-01-02)], ReadSchema: struct<product:string,revenue:double,date:string>
```

在这个输出中：

- `PushedFilters: [IsNotNull(date), EqualTo(date,2023-01-02)]` 表示 `pushedDownFilters`。这些过滤器已经下推到了数据源，在数据读取时应用。
- `DataFilters: [isnotnull(date#2), (date#2 = 2023-01-02)]` 表示在 Spark 执行计划中应用的过滤器。







## [配置项](https://spark.apache.org/docs/latest/configuration.html){:target="_blank"}

### Broadcast阈值

spark.sql.autoBroadcastJoinThreshold

设置为-1，则禁用Broadcast Hash Join
