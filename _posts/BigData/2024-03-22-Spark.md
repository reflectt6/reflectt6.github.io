---
layout: page-with-sidebar
title:  "Spark"
date:   2024-03-22 14:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
hideTag: false
---

## 安装部署

Spark有几种集群部署模式，具体请看[Cluster Mode Overview](https://spark.apache.org/docs/latest/cluster-overview.html)

比较关键的组件有Driver和Worker，见下图

![image-20240422143309188](/assets/images/2024-03-22-Spark//image-20240422143309188.png)

集中部署模式的区别在于Cluster Manager的不同

### Cluster Manager Types

The system currently supports several cluster managers:
系统目前支持多种集群管理器：

- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html) – a simple cluster manager included with Spark that makes it easy to set up a cluster.
  Standalone——Spark 附带的一个简单的集群管理器，可以轻松设置集群。

  要使用启动脚本启动 Spark 独立集群，您应该在 Spark 目录中创建一个名为 conf/workers 的文件，该文件必须包含您打算启动 Spark Worker 的所有计算机的主机名，每行一个。如果conf/workers不存在，启动脚本默认为单台机器（localhost），这对于测试很有用。请注意，主机通过 ssh 访问每台工作机。默认情况下，ssh 并行运行，并且需要设置无密码（使用私钥）访问。如果您没有无密码设置，则可以设置环境变量 SPARK_SSH_FOREGROUND 并连续为每个工作人员提供密码。

- [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)
  Apache Mesos – 通用集群管理器，还可以运行 Hadoop MapReduce 和服务应用程序。 （已弃用）

- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html) – the resource manager in Hadoop 3.
  Hadoop YARN – Hadoop 3 中的资源管理器。

- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) – an open-source system for automating deployment, scaling, and management of containerized applications.
  Kubernetes – 一个用于自动化部署、扩展和管理容器化应用程序的开源系统。

### [Submitting Applications](https://spark.apache.org/docs/latest/submitting-applications.html)

Spark通过shell脚本提交任务，使用脚本参数可以随意设置任务运行在哪一种部署模式下，常见参数如下：

- `--class`: The entry point for your application (e.g. `org.apache.spark.examples.SparkPi`)
  `--class` ：应用程序的入口点（例如 `org.apache.spark.examples.SparkPi` ）

- `--master`: The [master URL](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls) for the cluster (e.g. `spark://23.195.26.187:7077`)

  Master URLs有哪些呢？如下表

  The master URL passed to Spark can be in one of the following formats:

  | Master URL                        | Meaning                                                      |
  | --------------------------------- | ------------------------------------------------------------ |
  | `local`                           | Run Spark locally with one worker thread (i.e. no parallelism at all). |
  | `local[K]`                        | Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). |
  | `local[K,F]`                      | Run Spark locally with K worker threads and F maxFailures (see [spark.task.maxFailures](https://spark.apache.org/docs/latest/configuration.html#scheduling) for an explanation of this variable). |
  | `local[*]`                        | Run Spark locally with as many worker threads as logical cores on your machine. |
  | `local[*,F]`                      | Run Spark locally with as many worker threads as logical cores on your machine and F maxFailures. |
  | `local-cluster[N,C,M]`            | Local-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker. |
  | `spark://HOST:PORT`               | Connect to the given [Spark standalone cluster](https://spark.apache.org/docs/latest/spark-standalone.html) master. The port must be whichever one your master is configured to use, which is 7077 by default. |
  | `spark://HOST1:PORT1,HOST2:PORT2` | Connect to the given [Spark standalone cluster with standby masters with Zookeeper](https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper). The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default. |
  | `mesos://HOST:PORT`               | Connect to the given [Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use `mesos://zk://...`. To submit with `--deploy-mode cluster`, the HOST:PORT should be configured to connect to the [MesosClusterDispatcher](https://spark.apache.org/docs/latest/running-on-mesos.html#cluster-mode). |
  | `yarn`                            | Connect to a [YARN ](https://spark.apache.org/docs/latest/running-on-yarn.html)cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The cluster location will be found based on the `HADOOP_CONF_DIR` or `YARN_CONF_DIR` variable. |
  | `k8s://HOST:PORT`                 | Connect to a [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The `HOST` and `PORT` refer to the [Kubernetes API Server](https://kubernetes.io/docs/reference/generated/kube-apiserver/). It connects using TLS by default. In order to force it to use an unsecured connection, you can use `k8s://http://HOST:PORT`. |

- `--deploy-mode`: Whether to deploy your driver on the worker nodes (`cluster`) or locally as an external client (`client`) (default: `client`) **†**
  `--deploy-mode` ：是将驱动程序部署在工作节点 ( `cluster` ) 上还是作为外部客户端部署在本地 ( `client` )（默认值： `client` ) †

- `--conf`: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. `--conf <key>=<value> --conf <key2>=<value2>`)
  `--conf` ：任意 Spark 配置属性，采用 key=value 格式。对于包含空格的值，请将“key=value”括在引号中（如图所示）。多个配置应作为单独的参数传递。 （例如 `--conf <key>=<value> --conf <key2>=<value2>` ）

- `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.
  `application-jar` ：捆绑 jar 的路径，包括您的应用程序和所有依赖项。 URL 必须在集群内部全局可见，例如，所有节点上都存在的 `hdfs://` 路径或 `file://` 路径。

- `application-arguments`: Arguments passed to the main method of your main class, if any
  `application-arguments` ：传递给主类的 main 方法的参数（如果有）

## 常见部署问题

### 1、Spark Web UI访问不了

Spark Web UI可以看到历史执行的sql以及对应的执行计划、metrics等，对于开发很重要。假设Web UI的端口为18080，有时候我们无法访问他：

1、检查代理，在公司内网上外网需要代理，但是往往Spark部署在内网集群上，内网机器是不需要代理的。这时我们在windows上输入“代理”，打开代理服务器设置。在`请勿对以下列条目开头的地址使用代理服务器。。。`后面加上内网机器的IP。

2、检查集群里的HistoryServer有没有启动：

HistoryServer这个进程就是Web UI的服务进程，如果没有启动，是肯定访问不了web ui的。

配置history server：主要分两部分

- 第一部分：配置spark-defaults.conf

  ```conf
  spark.eventLog.enabled   true
  spark.eventLog.dir       hdfs://node7-2:8020/directory
  ```

  这个设置表示打开spark的日志系统，并设置日志输出目录。（没有日志输出，web ui也是用不了，web ui本质上是把磁盘的log文件可视化）

- 第二部分：设置spark-env.sh

  ```bash
  
   export SPARK_HISTORY_OPTS="
  -Dspark.history.ui.port=18080
  -Dspark.history.retainedApplications=30
  -Dspark.history.fs.logDirectory=hdfs://node7-2:8020/directory"
  ```

  这一部分设置了history server要去哪里找spark的日志（和上面的日志输出路径保持一致即可）

配置结束了，现在去spark/sbin目录下，运行 start-history-server.sh 即可拉起web ui服务。

[参考资料](https://blog.csdn.net/XIAOMO__/article/details/107988164)



## 源码特性

### 1、whole stage code gen

Spark中的Whole-Stage Code Generation（整阶段代码生成）是一种优化技术，用于提高Spark SQL查询的性能。它的主要思想是将一整个查询阶段的代码生成为一个单独的函数，以减少函数调用和数据序列化/反序列化的开销。

在Spark中，查询通常由多个操作组成，例如选择、过滤、聚合等。每个操作都可以表示为一个函数，但这些函数通常会导致较多的函数调用开销和数据的序列化/反序列化开销。为了优化这一点，Spark使用Whole-Stage Code Generation将多个操作合并为一个函数，并生成高效的Java字节码来执行这个函数。

整阶段代码生成的过程大致如下：

1. Spark SQL将逻辑查询计划转换为物理查询计划。
2. 物理查询计划被划分为一系列称为“物理子计划”的阶段，每个子计划包含一组可以一起执行的操作。
3. 对于每个物理子计划，Spark生成一个包含所有操作的单个函数，并为该函数生成优化的Java字节码。
4. 在运行时，生成的函数被传递给Spark执行引擎执行，从而减少了函数调用和数据的序列化/反序列化开销。

整阶段代码生成可以大大提高Spark SQL查询的性能，特别是在涉及大规模数据处理时。通过减少函数调用和数据的序列化/反序列化开销，整阶段代码生成可以显著提升查询的执行速度。

### 2、cloumnar

在Spark中，"columnar" 通常用于描述一种数据存储和处理的方式，即按列存储。与传统的按行存储相比，按列存储将同一列的数据连续存储在一起，而不是将整行数据连续存储。这种存储方式在数据分析和处理中具有一些优势，特别是对于大规模数据集。

在 Spark SQL 中，列式存储通常指的是 Parquet 格式。Parquet 是一种列式存储格式，它将数据按列存储在文件中，这样做有几个好处：

1. **高压缩率**：相同的值在列中是连续的，这样可以更容易地实现高效的压缩，减少存储空间的使用。
2. **只读查询效率高**：当查询只需要访问部分列时，只需要读取这些列的数据，而不必读取整行数据，这可以减少读取的数据量，提高查询效率。
3. **谓词下推**：列式存储使得更容易进行谓词下推，即在数据存储时就可以排除不满足条件的数据，减少查询时需要处理的数据量。

Spark SQL 在处理 Parquet 格式的数据时，会利用列式存储的优势来提高查询性能和降低资源消耗。例如，当使用整阶段代码生成来执行查询时，Spark 可以有效地处理列式存储的数据，从而加速查询的执行。

总的来说，列式存储在 Spark 中被广泛应用于大规模数据处理场景，可以提供更高效的数据存储和处理方式，特别是对于分析型工作负载。

### 3、AQE

在Apache Spark中，AQE（Adaptive Query Execution，自适应查询执行）是一种优化技术，旨在根据作业的运行情况动态地调整执行计划，以提高性能并降低资源消耗。AQE可以在运行时根据数据的分布、任务的执行情况和资源可用性等因素进行优化，从而更好地适应不同的工作负载和环境。

AQE的主要特点和功能包括：

1. **动态优化执行计划**：根据作业的运行情况，AQE可以动态地调整执行计划，例如更改连接策略、重新分区或更改过滤器等，以提高查询性能。
2. **自适应分区**：根据数据分布和操作的需要，AQE可以自动调整数据的分区方式，以减少数据倾斜并提高并行性。
3. **动态过滤器推断**：AQE可以根据运行时数据的统计信息推断出更好的过滤器，以减少需要处理的数据量。
4. **自适应扫描**：根据数据分布和过滤器条件，AQE可以选择合适的扫描方式，例如跳过不需要的数据块或者选择更有效的扫描方式。
5. **动态重分区**：根据数据倾斜和任务执行情况，AQE可以动态地重新分区数据，以平衡任务负载并提高并行性能。

通过这些功能，AQE可以在不同的查询场景下自动调整执行计划，从而提高查询性能和资源利用率。AQE是Spark 3.0中引入的功能，并在后续版本中不断改进和优化，成为了Spark SQL中重要的优化技术之一。

### 4、Origin

Orgin用于存储行号、sql文本、起止位置等

CurrentOrigin中有一个value变量，本质是一个LocalThread[Origin]

常看到下面的代码：

```scala
CurrentOrigin.withOrigin(origin) {
	 要执行的操作
}
// withOrigin做的事情是，如果“要执行的操作”正常执行（没有报错，没有try到异常），则设置新的orgin，否则恢复原来的origin。
```

`CurrentOrigin.withOrigin` 是 Spark 中的一个方法，用于在当前线程上设置一个新的 SQL 分析上下文（`SQLContext`）的 origin。在 Spark 中，origin 用于跟踪和诊断 SQL 查询或操作的来源，以便在日志或错误消息中提供更多的上下文信息。

该方法的签名通常是这样的：

```scala
def withOrigin[T](origin: Origin)(f: => T): T
```

其中，`origin` 是一个表示查询或操作来源的对象，通常包含文件名、行号等信息。`f` 是一个函数，表示需要在设置了新的 origin 的上下文中执行的代码块。该方法会返回 `f` 的结果。

使用 `CurrentOrigin.withOrigin` 方法可以在 Spark 中为特定的操作设置 origin，以便在日志中能够更容易地追踪和诊断问题。

### 5、ORC格式（列式存储）

ORC（Optimized Row Columnar）是一种优化的列式存储文件格式，通常用于大数据处理系统中，例如Apache Hive和Apache Spark等。ORC格式旨在提供高效的数据压缩和快速的读取性能，特别适用于大规模数据集的存储和分析。

ORC格式的主要特点和优势包括：

1. **列式存储**：将数据按列存储，相同类型的数据被连续地存储在一起，这样可以实现更好的压缩率和查询性能。
2. **分区存储**：数据可以按照用户定义的分区方式进行存储，这样可以提高查询效率，特别是在分析时只需要部分数据的情况下。
3. **压缩**：ORC格式支持多种压缩算法，例如Zlib、Snappy、LZO等，可以根据需要选择合适的压缩算法来减少存储空间。
4. **列统计信息**：ORC文件中包含了每列的统计信息，包括最小值、最大值、空值数量等，这些信息可以帮助查询优化器更好地执行查询计划。
5. **谓词下推**：ORC格式支持谓词下推（Predicate Pushdown），可以在读取数据时将过滤条件应用于存储文件，减少需要读取和处理的数据量。
6. **数据跳过**：ORC格式支持在读取数据时跳过不必要的行或列，从而进一步提高读取性能。

总的来说，ORC格式在大数据处理系统中被广泛应用，可以提供高效的数据存储和查询性能，特别适用于需要处理大规模数据集的场景。





## [配置项](https://spark.apache.org/docs/latest/configuration.html)

### Broadcast阈值

spark.sql.autoBroadcastJoinThreshold

设置为-1，则禁用Broadcast Hash Join
