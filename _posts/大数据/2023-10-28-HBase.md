---
layout: page-with-sidebar
title:  "HBase"
date:   2023-10-28 10:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
---

## PE

pe是hbase官方提供的一个测试工具，可以方便的创建测试数据集。

```shell
hbase pe --nomapred --oneCon=true --table=Tab_2G_write_20 --size=2 --valueSize=20 --columns=8 --presplit=10 --compress='SNAPPY' sequentialWrite 16
// 16线程 10个预分区 8个列 2G value长度为20 压缩方式为SNAPPY
```



## [官网资料](https://hbase.apache.org/apache_hbase_reference_guide.pdf)

学习hbase，当然首选官网了。官网有一版中文的资料，但是已经相当久了，只能图一乐。

看英文文档又非常痛苦，还好现在是AI的时代，chrom里装一个 `沉浸式翻译`插件就可以愉快的读native文档了。

### 架构

RegionServer管理着多个region，建议一个RegionServer管理region的数量不超过100个，因为假如一个RegionServer管理1000个region，那么就需要3.9G的MemStore空间，这还是啥都没存的情况下。（见72.1）

region又有以下结构（见72）

![image-20231120171009295](/assets/images/2023-10-28-HBase//image-20231120171009295.png)

一个Store存着0到多个StoreFile（HFile）（72.7）

这里辟个谣，StoreFile和Hfile是一个东西，很多资料里面说HFile属于StoreFile，这个是不准确的。而StoreFile/HFile里面是Block。

可以使用hbase hfile工具查看HFile信息, 可以通过以下命令查看工具用法

```shell
hbase hfile
```

打印如下：

```shell
usage: HFile [-a] [-b] [-e] [-f <arg> | -r <arg>] [-h] [-k] [-m] [-p]
       [-s] [-v] [-w <arg>]
 -a,--checkfamily         Enable family check
 -b,--printblocks         Print block index meta data
 -e,--printkey            Print keys
 -f,--file <arg>          File to scan. Pass full-path; e.g.
                          hdfs://a:9000/hbase/hbase:meta/12/34
 -h,--printblockheaders   Print block headers for each block.
 -k,--checkrow            Enable row order check; looks for out-of-order
                          keys
 -m,--printmeta           Print meta data of file
 -p,--printkv             Print key/value pairs
 -r,--region <arg>        Region to scan. Pass region name; e.g.
                          'hbase:meta,,1'
 -s,--stats               Print statistics
 -v,--verbose             Verbose output; emits file and meta data
                          delimiters
 -w,--seekToRow <arg>     Seek to this row and print all the kvs for this
                          row only
```

HFile中包含索引（见71.4.4）、bloom过滤器，以及实际的row

`An HFile is the file format that HBase uses to store data in HDFS. It contains a multi-layered
index which allows HBase to seek the data without having to read the whole file. `

关于HFile的详细介绍，可以看下[这一篇](https://www.iteye.com/blog/bit1129-2201094)

### Bloom过滤器

见124.4

存储在HFile的元数据中，下面的命令可以看到hfile的元数据，里面有Bloom Filter，也可以看到Block index的大小信息，

```shell
hbase hfile -m -f <hfile的全路径>
```

#### 源码分析

UT起手，根据官方文档得知，Bloom是属于HFile的一部分，所以HFile的UT里面肯定有相关测试。

分析TestHFile文件中的第一个UT `testReaderWithoutBlockCache`可以清晰的看到，这个UT使用`writeStoreFile()`随机生成了一些key value，并且通过StoreFileWriter的append方法写入了他们。

往下追踪正式进入HBase写流程

```java
// StoreFileWriter.java
@Override
public void append(final Cell cell) throws IOException {
  appendGeneralBloomfilter(cell);
  appendDeleteFamilyBloomFilter(cell);
  writer.append(cell);
  trackTimestamps(cell);
}
```

可以看到在writer写入之前，先计算了bloom filter。这里有两个filter，第二个是用于过滤deleted family的，不知道这是干啥的，可以先不管。

追踪appendGeneralBloomfilter

```java
private void appendGeneralBloomfilter(final Cell cell) throws IOException {
  if (this.generalBloomFilterWriter != null) {
    /*
     * http://2.bp.blogspot.com/_Cib_A77V54U/StZMrzaKufI/AAAAAAAAADo/ZhK7bGoJdMQ/s400/KeyValue.png
     * Key = RowLen + Row + FamilyLen + Column [Family + Qualifier] + Timestamp 3 Types of
     * Filtering: 1. Row = Row 2. RowCol = Row + Qualifier 3. RowPrefixFixedLength = Fixed Length
     * Row Prefix
     */
    bloomContext.writeBloom(cell);
  }
}
```

追踪writeBloom

```java
public void writeBloom(Cell cell) throws IOException {
  // only add to the bloom filter on a new, unique key
  if (isNewKey(cell)) {
    sanityCheck(cell);
    bloomFilterWriter.append(cell);
  }
}
```

追踪bloomFilterWriter.append

```java
// CompoundBloomFilterWriter.java
@Override
public void append(Cell cell) throws IOException {
  Objects.requireNonNull(cell);

  enqueueReadyChunk(false);

  if (chunk == null) {
    if (firstKeyInChunk != null) {
      throw new IllegalStateException(
        "First key in chunk already set: " + Bytes.toStringBinary(firstKeyInChunk));
    }
    // This will be done only once per chunk
    if (bloomType == BloomType.ROWCOL) {
      firstKeyInChunk = PrivateCellUtil
        .getCellKeySerializedAsKeyValueKey(PrivateCellUtil.createFirstOnRowCol(cell));
    } else {
      firstKeyInChunk = CellUtil.copyRow(cell);
    }
    allocateNewChunk();
  }

  chunk.add(cell);
  this.prevCell = cell;
  ++totalKeyCount;
}
```

可以看到关键操作是chunk.add(cell)，继续追踪

```java
// BloomFilterChunk.java
public void add(Cell cell) {
  /*
   * For faster hashing, use combinatorial generation
   * http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
   */
  int hash1;
  int hash2;
  HashKey<Cell> hashKey;
  if (this.bloomType == BloomType.ROWCOL) {
    hashKey = new RowColBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  } else {
    hashKey = new RowBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  }
  setHashLoc(hash1, hash2);
}
```

这个函数和里面的setHashLoc就包含了生成bloom的关键函数，能找到这里，原理也就看懂了。

但是思考一个问题，这个bloom只是add到了chunk中，但是最终我们肯定是要存到磁盘上的，那么chunk中的数据什么时候写入到磁盘的呢？

通过对StoreFileWriter的源码进行分析，我们可以知道在close方法中，bloom被写入到writer中，这里的writer实际上是一个HFileWriterImpl对象：

```java
private boolean closeGeneralBloomFilter() throws IOException {
  boolean hasGeneralBloom = closeBloomFilter(generalBloomFilterWriter);

  // add the general Bloom filter writer and append file info
  if (hasGeneralBloom) {
    writer.addGeneralBloomFilter(generalBloomFilterWriter);
    writer.appendFileInfo(BLOOM_FILTER_TYPE_KEY, Bytes.toBytes(bloomType.toString()));
    if (bloomParam != null) {
      writer.appendFileInfo(BLOOM_FILTER_PARAM_KEY, bloomParam);
    }
    bloomContext.addLastBloomKey(writer);
  }
  return hasGeneralBloom;
}
```

在HFileWriterImpl类中的close方法中，之前添加到wirter的additionalLoadOnOpenData（里面存着的就是bloom filter的wirter）写入磁盘

```java
for (BlockWritable w : additionalLoadOnOpenData) {
  blockWriter.writeBlock(w, outputStream);
  totalUncompressedBytes += blockWriter.getUncompressedSizeWithHeader();
}
// 还有其他很多信息落盘，这里都省略了
```



bloom读流程:

通过UT中的readStoreFile可以看出如何读取hfile中的block

```java
private void readStoreFile(Path storeFilePath, Configuration conf, ByteBuffAllocator alloc)
  throws Exception {
  // Open the file reader with block cache disabled.
  CacheConfig cache = new CacheConfig(conf, null, null, alloc);
  HFile.Reader reader = HFile.createReader(fs, storeFilePath, cache, true, conf);
  long offset = 0;
  while (offset < reader.getTrailer().getLoadOnOpenDataOffset()) {
    HFileBlock block = reader.readBlock(offset, -1, false, true, false, true, null, null);
    offset += block.getOnDiskSizeWithHeader();
    block.release(); // return back the ByteBuffer back to allocator.
  }
  reader.close();
}
```

这里我调试过了, reader.readBlock读出来的block包含各种类型，具体有哪几种类型的block，见BlockType.java



#### BloomFilterChunk

这个类是HBase实现bloom filter的核心类，这算是存储Bloom的比较底层的的类了，但不是最底层，最底层的是隶属于这个类的成员对象`protected ByteBuffer bloom`，也就是说bloom最底层是NIO数组。

但是BloomFilterChunk仍然是bloom最重要的类，接下来我讲讲理由

对于每一个BloomFilterChunk有：

成员函数：

keyCount：已经添加的key的数量

maxKeys：最大可以添加key的数量

那么这个最大数量是怎么来的呢？

```java
// BloomFilterUtil.java

/**
 * Creates a Bloom filter chunk of the given size.
 * @param byteSizeHint the desired number of bytes for the Bloom filter bit array. Will be
 *                     increased so that folding is possible.
 * @param errorRate    target false positive rate of the Bloom filter
 * @param hashType     Bloom filter hash function type
 * @return the new Bloom filter of the desired size
 */
public static BloomFilterChunk createBySize(int byteSizeHint, double errorRate, int hashType,
  int foldFactor, BloomType bloomType) {
  BloomFilterChunk bbf = new BloomFilterChunk(hashType, bloomType);

  bbf.byteSize = computeFoldableByteSize(byteSizeHint * 8L, foldFactor);
  long bitSize = bbf.byteSize * 8;
  bbf.maxKeys = (int) idealMaxKeys(bitSize, errorRate);
  bbf.hashCount = optimalFunctionCount(bbf.maxKeys, bitSize);

  // Adjust max keys to bring error rate closer to what was requested,
  // because byteSize was adjusted to allow for folding, and hashCount was
  // rounded.
  bbf.maxKeys = (int) computeMaxKeys(bitSize, errorRate, bbf.hashCount);

  return bbf;
}
```

从这个静态方法中我们可以看到，chunk的maxKeys其实是由 `bloom数组推荐的长度(byteSizeHint)`、`折叠率(foldFactor)`、`错误率(errorRate)`计算而来的。

我们继续，当HBase插入的key的数量大于bloom最大能存储的key的数量的时候，HBase不会进行扩容，而是会再新建一个chunk来接着存储。而之前已经存满的chunk会存入队列中。

```java
// CompoundBloomFilterWriter.java
@Override
public void append(Cell cell) throws IOException {
  Objects.requireNonNull(cell);

  enqueueReadyChunk(false);

  if (chunk == null) {
    if (firstKeyInChunk != null) {
      throw new IllegalStateException(
        "First key in chunk already set: " + Bytes.toStringBinary(firstKeyInChunk));
    }
    // This will be done only once per chunk
    if (bloomType == BloomType.ROWCOL) {
      firstKeyInChunk = PrivateCellUtil
        .getCellKeySerializedAsKeyValueKey(PrivateCellUtil.createFirstOnRowCol(cell));
    } else {
      firstKeyInChunk = CellUtil.copyRow(cell);
    }
    allocateNewChunk();
  }

  chunk.add(cell);
  this.prevCell = cell;
  ++totalKeyCount;
}
```

我们可以看到，在每次Bloom添加key前，会执行`enqueueReadyChunk(false)`，用处在于判断chunk是否存满，是否要新建一个chunk继续存储。

还有一点需要注意，存入队列之前，会对chunk进行压缩，在考虑Bloom的内存占用是需要考虑进来，具体见下面代码的注释

```java
// CompoundBloomFilterWriter.java
private void enqueueReadyChunk(boolean closing) {
  if (chunk == null || (chunk.getKeyCount() < chunk.getMaxKeys() && !closing)) {
    return;
  }

  if (firstKeyInChunk == null) {
    throw new NullPointerException(
      "Trying to enqueue a chunk, " + "but first key is null: closing=" + closing + ", keyCount="
        + chunk.getKeyCount() + ", maxKeys=" + chunk.getMaxKeys());
  }

  ReadyChunk readyChunk = new ReadyChunk();
  readyChunk.chunkId = numChunks - 1;
  readyChunk.chunk = chunk;
  readyChunk.firstKey = firstKeyInChunk;
  readyChunks.add(readyChunk);

  long prevMaxKeys = chunk.getMaxKeys();
  long prevByteSize = chunk.getByteSize();

  chunk.compactBloom(); // 内存压缩

  if (LOG.isTraceEnabled() && prevByteSize != chunk.getByteSize()) {
    LOG.trace("Compacted Bloom chunk #" + readyChunk.chunkId + " from [" + prevMaxKeys
      + " max keys, " + prevByteSize + " bytes] to [" + chunk.getMaxKeys() + " max keys, "
      + chunk.getByteSize() + " bytes]");
  }

  totalMaxKeys += chunk.getMaxKeys();
  totalByteSize += chunk.getByteSize();

  firstKeyInChunk = null;
  prevChunk = chunk;
  chunk = null;
}
```

明确了这一点，我们接着往下看，之前已经分析过chunk.add(cell)

```java
// BloomFilterChunk.java
public void add(Cell cell) {
  /*
   * For faster hashing, use combinatorial generation
   * http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
   */
  int hash1;
  int hash2;
  HashKey<Cell> hashKey;
  if (this.bloomType == BloomType.ROWCOL) {
    hashKey = new RowColBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  } else {
    hashKey = new RowBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  }
  setHashLoc(hash1, hash2);
}
```

现在我们看看细节，在setHashLoc之前这些hash类型是怎么实现的？



#### Hash Type

HBase中bloom实现有三种可替换的hash函数，分别为：

- Jenkins Hash
- Murmur Hash
- Murmur Hash3

默认是Murmur Hash, 可通过"hbase.hash.type"配置

可以通过以下源码得知：

```java
// Hash.java
public static int getHashType(Configuration conf) {
  String name = conf.get("hbase.hash.type", "murmur");
  return parseHashType(name);
}
```



那么我们可以得到HBase默认的hash函数murmur

```java
// MurmurHash.java
@Override
public <T> int hash(HashKey<T> hashKey, int seed) {
  int m = 0x5bd1e995;
  int r = 24;
  int length = hashKey.length();
  int h = seed ^ length;

  int len_4 = length >> 2;

  for (int i = 0; i < len_4; i++) {
    int i_4 = (i << 2);
    int k = hashKey.get(i_4 + 3);
    k = k << 8;
    k = k | (hashKey.get(i_4 + 2) & 0xff);
    k = k << 8;
    k = k | (hashKey.get(i_4 + 1) & 0xff);
    k = k << 8;
    // noinspection PointlessArithmeticExpression
    k = k | (hashKey.get(i_4 + 0) & 0xff);
    k *= m;
    k ^= k >>> r;
    k *= m;
    h *= m;
    h ^= k;
  }

  // avoid calculating modulo
  int len_m = len_4 << 2;
  int left = length - len_m;
  int i_m = len_m;

  if (left != 0) {
    if (left >= 3) {
      h ^= hashKey.get(i_m + 2) << 16;
    }
    if (left >= 2) {
      h ^= hashKey.get(i_m + 1) << 8;
    }
    if (left >= 1) {
      h ^= hashKey.get(i_m);
    }

    h *= m;
  }

  h ^= h >>> 13;
  h *= m;
  h ^= h >>> 15;

  return h;
}
```

至于我为什么要分析bloom的实现，原因在于我在预研ribbon filter去替换hbase中的bloom filter。为了证明ribbon filter确实比bloom filter更好，我需要使用c++重新实现bloom filter然后与rocksdb中的ribbon filter做对比测试。

至于为什么不用java实现ribbon filter，然后和hbase中的bloom做对比。当然是ribbon的实现太复杂啦。能调api，谁想去造轮子呢？

这部分内容请看 [预研](/大数据/2023/11/23/过滤器优化.html)
