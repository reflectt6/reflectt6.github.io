---
layout: page-with-sidebar
title:  "HBase"
date:   2023-10-28 10:47:03 +0800
author: reflectt6
categories: "大数据"
#permalink: 
mainTag: "大数据"
secondaryTag: ""
---

## PE

pe是hbase官方提供的一个测试工具，可以方便的创建测试数据集。

```shell
hbase pe --nomapred --oneCon=true --table=Tab_2G_write_20 --size=2 --valueSize=20 --columns=8 --presplit=10 --compress='SNAPPY' sequentialWrite 16
// 16线程 10个预分区 8个列 2G value长度为20 压缩方式为SNAPPY
```



## [官网资料](https://hbase.apache.org/apache_hbase_reference_guide.pdf)

学习hbase，当然首选官网了。官网有一版中文的资料，但是已经相当久了，只能图一乐。

看英文文档又非常痛苦，还好现在是AI的时代，chrom里装一个 `沉浸式翻译`插件就可以愉快的读native文档了。

### 架构

RegionServer管理着多个region，建议一个RegionServer管理region的数量不超过100个，因为假如一个RegionServer管理1000个region，那么就需要3.9G的MemStore空间，这还是啥都没存的情况下。（见72.1）

region又有以下结构（见72）

![image-20231120171009295](/assets/images/2023-10-28-HBase//image-20231120171009295.png)

一个Store存着0到多个StoreFile（HFile）（72.7）

这里辟个谣，StoreFile和Hfile是一个东西，很多资料里面说HFile属于StoreFile，这个是不准确的。而StoreFile/HFile里面是Block。

可以使用hbase hfile工具查看HFile信息, 可以通过以下命令查看工具用法

```shell
hbase hfile
```

打印如下：

```shell
usage: HFile [-a] [-b] [-e] [-f <arg> | -r <arg>] [-h] [-k] [-m] [-p]
       [-s] [-v] [-w <arg>]
 -a,--checkfamily         Enable family check
 -b,--printblocks         Print block index meta data
 -e,--printkey            Print keys
 -f,--file <arg>          File to scan. Pass full-path; e.g.
                          hdfs://a:9000/hbase/hbase:meta/12/34
 -h,--printblockheaders   Print block headers for each block.
 -k,--checkrow            Enable row order check; looks for out-of-order
                          keys
 -m,--printmeta           Print meta data of file
 -p,--printkv             Print key/value pairs
 -r,--region <arg>        Region to scan. Pass region name; e.g.
                          'hbase:meta,,1'
 -s,--stats               Print statistics
 -v,--verbose             Verbose output; emits file and meta data
                          delimiters
 -w,--seekToRow <arg>     Seek to this row and print all the kvs for this
                          row only
```

HFile中包含索引（见71.4.4）、bloom过滤器，以及实际的row

`An HFile is the file format that HBase uses to store data in HDFS. It contains a multi-layered
index which allows HBase to seek the data without having to read the whole file. `

关于HFile的详细介绍，可以看下[这一篇](https://www.iteye.com/blog/bit1129-2201094)

### Bloom过滤器

见124.4

存储在HFile的元数据中，下面的命令可以看到hfile的元数据，里面有Bloom Filter，也可以看到Block index的大小信息，

```shell
hbase hfile -m -f <hfile的全路径>
```

#### 源码分析

UT起手，根据官方文档得知，Bloom是属于HFile的一部分，所以HFile的UT里面肯定有相关测试。

分析TestHFile文件中的第一个UT `testReaderWithoutBlockCache`可以清晰的看到，这个UT使用`writeStoreFile()`随机生成了一些key value，并且通过StoreFileWriter的append方法写入了他们。

往下追踪正式进入HBase写流程

```java
// StoreFileWriter.java
@Override
public void append(final Cell cell) throws IOException {
  appendGeneralBloomfilter(cell);
  appendDeleteFamilyBloomFilter(cell);
  writer.append(cell);
  trackTimestamps(cell);
}
```

可以看到在writer写入之前，先计算了bloom filter。这里有两个filter，第二个是用于过滤deleted family的，不知道这是干啥的，可以先不管。

追踪appendGeneralBloomfilter

```java
private void appendGeneralBloomfilter(final Cell cell) throws IOException {
  if (this.generalBloomFilterWriter != null) {
    /*
     * http://2.bp.blogspot.com/_Cib_A77V54U/StZMrzaKufI/AAAAAAAAADo/ZhK7bGoJdMQ/s400/KeyValue.png
     * Key = RowLen + Row + FamilyLen + Column [Family + Qualifier] + Timestamp 3 Types of
     * Filtering: 1. Row = Row 2. RowCol = Row + Qualifier 3. RowPrefixFixedLength = Fixed Length
     * Row Prefix
     */
    bloomContext.writeBloom(cell);
  }
}
```

追踪writeBloom

```java
public void writeBloom(Cell cell) throws IOException {
  // only add to the bloom filter on a new, unique key
  if (isNewKey(cell)) {
    sanityCheck(cell);
    bloomFilterWriter.append(cell);
  }
}
```

追踪bloomFilterWriter.append

```java
// CompoundBloomFilterWriter.java
@Override
public void append(Cell cell) throws IOException {
  Objects.requireNonNull(cell);

  enqueueReadyChunk(false);

  if (chunk == null) {
    if (firstKeyInChunk != null) {
      throw new IllegalStateException(
        "First key in chunk already set: " + Bytes.toStringBinary(firstKeyInChunk));
    }
    // This will be done only once per chunk
    if (bloomType == BloomType.ROWCOL) {
      firstKeyInChunk = PrivateCellUtil
        .getCellKeySerializedAsKeyValueKey(PrivateCellUtil.createFirstOnRowCol(cell));
    } else {
      firstKeyInChunk = CellUtil.copyRow(cell);
    }
    allocateNewChunk();
  }

  chunk.add(cell);
  this.prevCell = cell;
  ++totalKeyCount;
}
```

可以看到关键操作是chunk.add(cell)，继续追踪

```java
// BloomFilterChunk.java
public void add(Cell cell) {
  /*
   * For faster hashing, use combinatorial generation
   * http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
   */
  int hash1;
  int hash2;
  HashKey<Cell> hashKey;
  if (this.bloomType == BloomType.ROWCOL) {
    hashKey = new RowColBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  } else {
    hashKey = new RowBloomHashKey(cell);
    hash1 = this.hash.hash(hashKey, 0);
    hash2 = this.hash.hash(hashKey, hash1);
  }
  setHashLoc(hash1, hash2);
}
```

这个函数和里面的setHashLoc就包含了生成bloom的关键函数，能找到这里，原理也就看懂了。

但是思考一个问题，这个bloom只是add到了chunk中，但是最终我们肯定是要存到磁盘上的，那么chunk中的数据什么时候写入到磁盘的呢？

通过对StoreFileWriter的源码进行分析，我们可以知道在close方法中，bloom被写入到writer中，这里的writer实际上是一个HFileWriterImpl对象：

```java
private boolean closeGeneralBloomFilter() throws IOException {
  boolean hasGeneralBloom = closeBloomFilter(generalBloomFilterWriter);

  // add the general Bloom filter writer and append file info
  if (hasGeneralBloom) {
    writer.addGeneralBloomFilter(generalBloomFilterWriter);
    writer.appendFileInfo(BLOOM_FILTER_TYPE_KEY, Bytes.toBytes(bloomType.toString()));
    if (bloomParam != null) {
      writer.appendFileInfo(BLOOM_FILTER_PARAM_KEY, bloomParam);
    }
    bloomContext.addLastBloomKey(writer);
  }
  return hasGeneralBloom;
}
```

在HFileWriterImpl类中的close方法中，之前添加到wirter的additionalLoadOnOpenData（里面存着的就是bloom filter的wirter）写入磁盘

```java
for (BlockWritable w : additionalLoadOnOpenData) {
  blockWriter.writeBlock(w, outputStream);
  totalUncompressedBytes += blockWriter.getUncompressedSizeWithHeader();
}
// 还有其他很多信息落盘，这里都省略了
```



bloom读流程:

通过UT中的readStoreFile可以看出如何读取hfile中的block

```java
private void readStoreFile(Path storeFilePath, Configuration conf, ByteBuffAllocator alloc)
  throws Exception {
  // Open the file reader with block cache disabled.
  CacheConfig cache = new CacheConfig(conf, null, null, alloc);
  HFile.Reader reader = HFile.createReader(fs, storeFilePath, cache, true, conf);
  long offset = 0;
  while (offset < reader.getTrailer().getLoadOnOpenDataOffset()) {
    HFileBlock block = reader.readBlock(offset, -1, false, true, false, true, null, null);
    offset += block.getOnDiskSizeWithHeader();
    block.release(); // return back the ByteBuffer back to allocator.
  }
  reader.close();
}
```

这里我调试过了, reader.readBlock读出来的block包含各种类型，具体有哪几种类型的block，见BlockType.java



