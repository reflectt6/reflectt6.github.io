---
layout: page-with-sidebar
title:  "JvmGC问题分析"
date:   2023-10-25 10:47:03 +0800
author: reflectt6
categories: "Java"
mainTag: "Java"
secondaryTag: "JVM"
hideTag: false
---

## 背景

工作中遇到了一个由于GC导致任务失败的问题。背景是hbase二级索引的一致性校验工具。该工具会启动若干个map任务和一个reduce任务，对数据表和索引表的一致性进行校验。

在对1T数据表做校验时，运行12小时左右，reduce任务失败，导致reduce任务重新执行，反复失败3次之后，整个任务报错。

reduce报错有 `GC overhead limit exceeded的`字样

## 实战

1、由于reduce任务失败之后会反复重试，所以我们有充分的时间对其进行打印分析，例如

```shell
jmap -heap pid

jstat -gc pid
```

其中就包含了GC的次数，堆内存的占用情况。

如果GC次数过多，在没有充分调优的情况下，想到了加大mapreduce任务的堆内存，减少gc次数。

这本次实战中，我观察到老年代已经满了，Eden也满了，没有内存给reduce程序用了。对此进行合理猜测，因为map任务实际上最后生成了一个GlobalHindexConsistencyJobRecord对象，记录了一条hbase row的检查情况。reduce要对所有map的JobRecord的对象进行合并，这里可能是由于JobRecord对象太多了，导致reduce存储不下了。

观察集群资源：

```shell
free -h
```

发现每个节点有350G左右的空闲内存，于是在资源足够的情况下，首先想到增加堆内存。

对于mapreduce任务需要修改hadoop/etc/hadoop/mapred-site.xml中的

```
// reduce 程序总的内存 单位mb
mapreduce.reduce.memory.mb
// reduce java选项，可通过-Xms设置堆最小值，-Xmx设置堆最大值，推荐堆最大值为程序总内存的80%
mapreduce.reduce.java.opts
```

给点其他的mapreduce调优参数

```
// 同上，一个道理
mapreduce.map.memory.mb
mapreduce.reduce.memory.mb
mapreduce.map.java.opts
mapreduce.reduce.java.opts

// 忘了
mapred.reduce.parallel.copies
// map任务还剩多少完成，reduce任务就启动了。默认0.05，表示map任务在99.5%时，启动reduce任务。
mapreduce.job.reduce.slowstart.completedmaps
```



修改之后问题得到缓解，但是没有解决，后续解决方案见[《MapReduce调优》](/大数据/2023/11/02/MapReduce调优.html)中的调优策略章节









